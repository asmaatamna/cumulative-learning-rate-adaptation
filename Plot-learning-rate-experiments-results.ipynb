{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T13:28:29.385035Z",
     "start_time": "2025-07-21T13:28:29.022695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "%matplotlib qt"
   ],
   "id": "fa66c3b5861ac4f5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:29:59.355273Z",
     "start_time": "2025-07-18T17:29:59.343320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def summarize_metric_sweep_lr(base_dir, algorithms, datasets, lr_values, metric=\"test_accuracy\"):\n",
    "    \"\"\"\n",
    "    Computes mean and std of a specified metric (e.g., test_accuracy or test_loss) over seeds,\n",
    "    for each learning rate in `lr_values`. Displays results by algorithm (rows) and dataset (columns),\n",
    "    grouped by learning rate with the LR value shown only once per block.\n",
    "\n",
    "    Returns:\n",
    "        summary_stats: dict[lr][algorithm][dataset] = (mean, std)\n",
    "    \"\"\"\n",
    "\n",
    "    # folder_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2})_(\\d{2}-\\d{2})_(\\d+)_([\\d]+)\")\n",
    "    folder_pattern = re.compile(r\"(\\d+)_([\\d]+)_EpochSeed\")\n",
    "\n",
    "    # Store results: {lr -> algorithm -> dataset -> values}\n",
    "    summary_stats = defaultdict(lambda: defaultdict(dict))\n",
    "    all_rows = []\n",
    "\n",
    "    for lr_value in lr_values:\n",
    "        results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "        for entry in os.listdir(base_dir):\n",
    "            if not folder_pattern.match(entry):\n",
    "                continue\n",
    "            run_dir = os.path.join(base_dir, entry)\n",
    "\n",
    "            for dataset in datasets:\n",
    "                dataset_path = os.path.join(run_dir, dataset)\n",
    "                lr_folder = os.path.join(dataset_path, f\"lr{lr_value}\")\n",
    "                if not os.path.isdir(lr_folder):\n",
    "                    continue\n",
    "\n",
    "                for algorithm in algorithms:\n",
    "                    result_file = os.path.join(lr_folder, f\"{algorithm}.pkl\")\n",
    "                    if os.path.isfile(result_file):\n",
    "                        with open(result_file, 'rb') as f:\n",
    "                            data = pickle.load(f)\n",
    "                            val = data.get(metric)\n",
    "                            if val is not None:\n",
    "                                results[algorithm][dataset].append(val)\n",
    "\n",
    "        # Compute mean/std for this learning rate\n",
    "        for algorithm in algorithms:\n",
    "            row = [lr_value if algorithm == algorithms[0] else \"\", algorithm]\n",
    "            for dataset in datasets:\n",
    "                vals = results[algorithm][dataset]\n",
    "                if vals:\n",
    "                    mean_val = np.mean(vals)\n",
    "                    std_val = np.std(vals)\n",
    "                    summary_stats[lr_value][algorithm][dataset] = (mean_val, std_val)\n",
    "                    row.append(f\"{mean_val:.4f} ± {std_val:.4f}\")\n",
    "                else:\n",
    "                    summary_stats[lr_value][algorithm][dataset] = None\n",
    "                    row.append(\"-\")\n",
    "            all_rows.append(row)\n",
    "\n",
    "    # Build headers\n",
    "    headers = [\"LR\", \"Algorithm\"] + datasets\n",
    "    table_str = tabulate(all_rows, headers=headers, tablefmt=\"fancy_grid\")\n",
    "    print(table_str)\n",
    "    return summary_stats"
   ],
   "id": "f379bde004db3a6a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:30:00.447618Z",
     "start_time": "2025-07-18T17:30:00.442711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_algorithm_colors(algorithms):\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping each algorithm to a distinct color from the 'tab10' colormap.\n",
    "    \"\"\"\n",
    "\n",
    "    # num_colors = len(algorithms)\n",
    "    # colormap = plt.get_cmap('tab10', num_colors)\n",
    "    # color_map = {algorithm: colormap(i) for i, algorithm in enumerate(algorithms)}\n",
    "    color_map = {\n",
    "        \"SGD\": \"blue\",\n",
    "        \"SGD_CLARA\": \"dodgerblue\",\n",
    "        \"SGD_CLARA_us\": \"cyan\",\n",
    "\n",
    "        \"Adam\": \"red\",\n",
    "        \"Adam_CLARA\": \"coral\",  # \"deeppink\",\n",
    "        \"Adam_CLARA_us\": \"magenta\",\n",
    "\n",
    "        \"D-Adaptation\": \"green\"\n",
    "    }\n",
    "    return color_map"
   ],
   "id": "c4829c660974e782",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:30:02.323523Z",
     "start_time": "2025-07-18T17:30:02.314528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_algorithm_performance(summary_stats, algorithms, datasets, lr_values, metric_name=\"Test Accuracy\"):\n",
    "    \"\"\"\n",
    "    Plot bar charts with error bars showing mean and std for each algorithm at different learning rates,\n",
    "    one plot per dataset, with improved spacing, consistent y-axis, a horizontal line for best performance,\n",
    "    and transparency on non-best bars. Each algorithm is assigned a consistent color.\n",
    "    \"\"\"\n",
    "\n",
    "    num_lrs = len(lr_values)\n",
    "    num_algos = len(algorithms)\n",
    "    bar_width = 0.15\n",
    "    group_spacing = 0.4  # space between groups\n",
    "    total_group_width = num_algos * bar_width + group_spacing\n",
    "    x = np.arange(num_lrs) * total_group_width\n",
    "\n",
    "    color_map = get_algorithm_colors(algorithms)\n",
    "\n",
    "    for dataset in datasets:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        all_means = []  # store all means to compute the best one\n",
    "\n",
    "        # First, gather all means to find the best one\n",
    "        for algorithm in algorithms:\n",
    "            for lr in lr_values:\n",
    "                stats = summary_stats.get(lr, {}).get(algorithm, {}).get(dataset)\n",
    "                if stats is not None:\n",
    "                    mean, _ = stats\n",
    "                    all_means.append((mean, algorithm, lr))\n",
    "\n",
    "        if not all_means:\n",
    "            continue  # skip empty plots\n",
    "\n",
    "        # Find the best performing (mean) algorithm-lr pair\n",
    "        best_mean, best_algo, best_lr = max(all_means, key=lambda x: x[0])\n",
    "\n",
    "        for i, algorithm in enumerate(algorithms):\n",
    "            means = []\n",
    "            stds = []\n",
    "            alphas = []\n",
    "\n",
    "            for lr in lr_values:\n",
    "                stats = summary_stats.get(lr, {}).get(algorithm, {}).get(dataset)\n",
    "                if stats is not None:\n",
    "                    mean, std = stats\n",
    "                else:\n",
    "                    mean, std = 0, 0\n",
    "                means.append(mean)\n",
    "                stds.append(std)\n",
    "\n",
    "                # Make best bar opaque, others semi-transparent\n",
    "                if algorithm == best_algo and lr == best_lr:\n",
    "                    alphas.append(1.0)\n",
    "                else:\n",
    "                    alphas.append(0.5)\n",
    "\n",
    "            x_pos = x + i * bar_width\n",
    "            bars = plt.bar(x_pos, means, width=bar_width, label=algorithm, yerr=stds,\n",
    "                           capsize=5, color=color_map[algorithm])\n",
    "\n",
    "            # Adjust alpha bar by bar\n",
    "            for bar, a in zip(bars, alphas):\n",
    "                bar.set_alpha(a)\n",
    "\n",
    "        # Add horizontal line at best performance\n",
    "        plt.axhline(y=best_mean, color='gray', linestyle='--', linewidth=1)\n",
    "        plt.text(x[0] - bar_width * 3.9, best_mean, f\"{best_mean:.2f}\", va='center', ha='right', color='gray')\n",
    "\n",
    "        # Plot styling\n",
    "        plt.xticks(x + (num_algos / 2 - 0.5) * bar_width, lr_values)\n",
    "        plt.xlabel(\"Learning Rate\")\n",
    "        plt.ylabel(metric_name)\n",
    "        plt.title(f\"{metric_name} on {dataset}\")\n",
    "        plt.ylim(0, 100)\n",
    "        plt.legend()\n",
    "        plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "id": "5de6ddbc031580e6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:30:17.626094Z",
     "start_time": "2025-07-18T17:30:14.481074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summary = summarize_metric_sweep_lr(\n",
    "    base_dir=\"TM_Experiments/selected_results/\",\n",
    "    algorithms=[\"SGD\", \"SGD_CLARA\", \"SGD_CLARA_us\", \"Adam\", \"Adam_CLARA\", \"Adam_CLARA_us\", \"D-Adaptation\"],\n",
    "    datasets=[\"breast_cancer\", \"iris\", \"wine\", \"digits\", \"mnist\", \"fmnist\", \"cifar10\", \"cifar100\"],\n",
    "    lr_values=[\"1e-06\", \"1e-05\", \"1e-04\", \"1e-03\", \"1e-02\", \"1e-01\", \"1.00\"],  # must match folder name exactly\n",
    "    metric=\"test_accuracy\"  # test_accuracy or test_loss\n",
    ")\n"
   ],
   "id": "3e8d45ebebaddf99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════╤═══════════════╤═══════════════════╤═══════════════════╤═══════════════════╤══════════════════╤═══════════════════╤═══════════════════╤═══════════════════╤══════════════════╕\n",
      "│ LR    │ Algorithm     │ breast_cancer     │ iris              │ wine              │ digits           │ mnist             │ fmnist            │ cifar10           │ cifar100         │\n",
      "╞═══════╪═══════════════╪═══════════════════╪═══════════════════╪═══════════════════╪══════════════════╪═══════════════════╪═══════════════════╪═══════════════════╪══════════════════╡\n",
      "│ 1e-06 │ SGD           │ 35.0877 ± 28.1414 │ 28.0000 ± 17.5879 │ 25.0000 ± 16.0054 │ 9.8333 ± 3.2327  │ 11.6580 ± 2.1618  │ 13.1020 ± 6.1303  │ 10.0820 ± 0.1534  │ 0.9780 ± 0.1994  │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ SGD_CLARA     │ 92.4561 ± 0.8946  │ 71.3333 ± 10.0222 │ 53.8889 ± 16.2542 │ 95.1111 ± 0.7974 │ 93.1800 ± 0.1666  │ 84.5200 ± 0.1351  │ 58.8500 ± 0.9755  │ 32.0200 ± 0.7875 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ SGD_CLARA_us  │ 91.4035 ± 1.5092  │ 90.6667 ± 7.4237  │ 45.5556 ± 24.5075 │ 96.2222 ± 0.6713 │ 92.7040 ± 0.1652  │ 84.0660 ± 0.1382  │ 50.2880 ± 1.1166  │ 20.4120 ± 0.3384 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam          │ 42.4561 ± 18.6626 │ 30.6667 ± 19.4822 │ 28.3333 ± 10.8866 │ 10.1667 ± 2.7588 │ 90.1720 ± 0.1165  │ 81.0300 ± 0.0603  │ 50.0060 ± 0.4747  │ 17.6040 ± 0.4160 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam_CLARA    │ 93.6842 ± 0.6564  │ 31.3333 ± 18.8090 │ 36.6667 ± 17.0692 │ 93.7222 ± 0.8889 │ 94.0640 ± 0.3389  │ 84.9500 ± 0.1055  │ 55.8640 ± 0.5199  │ 29.8020 ± 0.9277 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam_CLARA_us │ 84.9123 ± 6.0875  │ 32.0000 ± 20.5047 │ 32.2222 ± 1.3608  │ 96.3333 ± 0.4779 │ 94.2900 ± 0.2608  │ 85.2700 ± 0.1734  │ 54.0940 ± 0.5501  │ 23.3620 ± 0.7970 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ D-Adaptation  │ 88.4211 ± 3.2063  │ 96.6667 ± 0.0000  │ 91.1111 ± 4.0825  │ 95.2222 ± 0.3239 │ 97.7660 ± 0.2760  │ 86.3780 ± 1.2217  │ 67.0640 ± 6.6233  │ 36.8540 ± 0.4963 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│ 1e-05 │ SGD           │ 75.7895 ± 10.4618 │ 28.0000 ± 17.5879 │ 25.5556 ± 19.4365 │ 14.4444 ± 4.5576 │ 36.8600 ± 4.8875  │ 40.9960 ± 4.4773  │ 11.3480 ± 1.3990  │ 1.0320 ± 0.2137  │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ SGD_CLARA     │ 88.7719 ± 3.2063  │ 81.3333 ± 8.5894  │ 49.4444 ± 6.6667  │ 95.0556 ± 0.7328 │ 93.1760 ± 0.2051  │ 84.4960 ± 0.1695  │ 58.8040 ± 0.9698  │ 32.7000 ± 0.6255 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ SGD_CLARA_us  │ 90.8772 ± 2.1912  │ 94.0000 ± 2.4944  │ 51.6667 ± 22.5394 │ 96.2778 ± 0.6713 │ 92.7100 ± 0.1616  │ 84.0720 ± 0.1440  │ 50.3100 ± 1.1364  │ 21.1260 ± 0.3536 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam          │ 42.2807 ± 18.9474 │ 30.6667 ± 19.4822 │ 28.8889 ± 10.4822 │ 15.0000 ± 2.4088 │ 96.5600 ± 0.0772  │ 86.8200 ± 0.1213  │ 66.6700 ± 0.6443  │ 33.1300 ± 0.3754 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam_CLARA    │ 92.2807 ± 4.9435  │ 31.3333 ± 18.8090 │ 68.8889 ± 9.8445  │ 93.8333 ± 0.7115 │ 96.5160 ± 0.0840  │ 86.8100 ± 0.1121  │ 66.4460 ± 0.7213  │ 33.2020 ± 0.2845 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam_CLARA_us │ 88.5965 ± 0.7846  │ 32.0000 ± 18.2087 │ 41.6667 ± 18.9215 │ 96.3889 ± 0.4648 │ 94.5760 ± 0.2262  │ 85.4940 ± 0.1025  │ 54.6180 ± 0.5529  │ 23.3560 ± 0.2275 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ D-Adaptation  │ 89.8246 ± 2.8070  │ 96.6667 ± 0.0000  │ 95.0000 ± 1.1111  │ 95.4444 ± 0.5720 │ 97.6980 ± 0.3653  │ 86.2980 ± 0.3421  │ 70.8960 ± 3.4931  │ 35.4480 ± 1.3119 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│ 1e-04 │ SGD           │ 85.2632 ± 4.2397  │ 27.3333 ± 18.3060 │ 49.4444 ± 10.4527 │ 70.0000 ± 4.6481 │ 84.4520 ± 0.3954  │ 73.8260 ± 0.1366  │ 25.0920 ± 1.2717  │ 1.7880 ± 0.3779  │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ SGD_CLARA     │ 93.1579 ± 1.0230  │ 88.0000 ± 10.8730 │ 52.7778 ± 11.5202 │ 94.9444 ± 0.6894 │ 93.1980 ± 0.1808  │ 84.5240 ± 0.1439  │ 58.7960 ± 0.9538  │ 33.2620 ± 0.6330 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ SGD_CLARA_us  │ 91.5789 ± 1.3129  │ 91.3333 ± 5.4160  │ 45.5556 ± 26.4458 │ 96.2778 ± 0.6236 │ 92.7340 ± 0.1483  │ 84.1060 ± 0.1435  │ 50.4900 ± 1.1235  │ 21.4680 ± 0.2769 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam          │ 32.9825 ± 18.0932 │ 28.0000 ± 22.7645 │ 30.5556 ± 10.6863 │ 84.2222 ± 2.5784 │ 97.9880 ± 0.1907  │ 88.9880 ± 0.0950  │ 72.3080 ± 0.8508  │ 36.3620 ± 0.6390 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam_CLARA    │ 93.6842 ± 0.8595  │ 30.6667 ± 19.4822 │ 81.6667 ± 5.7198  │ 94.0000 ± 0.7370 │ 98.0440 ± 0.0383  │ 89.0680 ± 0.1463  │ 72.3480 ± 0.3553  │ 37.5580 ± 0.5757 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam_CLARA_us │ 87.7193 ± 0.7846  │ 31.3333 ± 18.2087 │ 67.7778 ± 9.3953  │ 96.3333 ± 0.4779 │ 94.9280 ± 0.2638  │ 85.7760 ± 0.0913  │ 55.4180 ± 0.5942  │ 25.2740 ± 0.4869 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ D-Adaptation  │ 89.1228 ± 3.6210  │ 96.6667 ± 0.0000  │ 91.1111 ± 2.7217  │ 95.2778 ± 0.6086 │ 97.6620 ± 0.1821  │ 87.2420 ± 0.4399  │ 67.2200 ± 7.5286  │ 33.8040 ± 2.9709 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│ 1e-03 │ SGD           │ 86.6667 ± 1.0230  │ 36.6667 ± 12.4722 │ 52.2222 ± 14.2075 │ 93.5556 ± 0.6667 │ 93.1380 ± 0.0906  │ 84.1240 ± 0.0816  │ 57.8600 ± 1.0617  │ 24.1300 ± 0.9645 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ SGD_CLARA     │ 93.6842 ± 0.6564  │ 78.6667 ± 22.6667 │ 61.6667 ± 7.7380  │ 95.1667 ± 0.7974 │ 94.0740 ± 0.1983  │ 85.2580 ± 0.1287  │ 62.8660 ± 0.4683  │ 33.6340 ± 0.3797 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ SGD_CLARA_us  │ 90.3509 ± 0.9609  │ 88.0000 ± 4.5216  │ 44.4444 ± 19.5631 │ 96.3333 ± 0.5932 │ 94.0880 ± 0.1338  │ 85.1000 ± 0.1431  │ 54.8220 ± 0.5622  │ 22.7600 ± 0.1734 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam          │ 87.7193 ± 5.4921  │ 38.6667 ± 25.6125 │ 54.4444 ± 18.3081 │ 97.2222 ± 0.4969 │ 98.1020 ± 0.1042  │ 88.8680 ± 0.2956  │ 73.8560 ± 0.3810  │ 36.3340 ± 0.4739 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam_CLARA    │ 95.4386 ± 0.6564  │ 80.6667 ± 8.2731  │ 88.3333 ± 3.2394  │ 97.3889 ± 0.5720 │ 98.2080 ± 0.1920  │ 89.3800 ± 0.1746  │ 75.5840 ± 0.4964  │ 39.6140 ± 0.5617 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam_CLARA_us │ 88.0702 ± 0.7018  │ 32.0000 ± 21.2498 │ 82.2222 ± 4.5134  │ 96.6667 ± 0.5556 │ 97.6920 ± 0.0830  │ 88.1760 ± 0.1480  │ 68.0600 ± 0.1879  │ 34.4800 ± 0.4909 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ D-Adaptation  │ 90.3509 ± 4.0006  │ 96.6667 ± 0.0000  │ 88.3333 ± 3.6851  │ 96.0556 ± 0.4082 │ 98.0640 ± 0.2642  │ 86.8460 ± 0.5859  │ 66.4900 ± 2.2752  │ 26.7500 ± 1.0928 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│ 1e-02 │ SGD           │ 83.6842 ± 3.4019  │ 54.0000 ± 15.9722 │ 57.7778 ± 9.1961  │ 95.9444 ± 0.2833 │ 97.8220 ± 0.0511  │ 88.5740 ± 0.2451  │ 69.6620 ± 0.7287  │ 35.6280 ± 0.3953 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ SGD_CLARA     │ 93.5088 ± 0.4297  │ 85.3333 ± 3.3993  │ 72.2222 ± 2.4845  │ 97.7778 ± 0.2485 │ 97.7980 ± 0.0483  │ 88.6540 ± 0.1476  │ 69.5560 ± 0.7392  │ 35.4140 ± 0.4459 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ SGD_CLARA_us  │ 90.0000 ± 1.3129  │ 88.0000 ± 4.9889  │ 45.0000 ± 3.6851  │ 97.2778 ± 0.5386 │ 97.8540 ± 0.0781  │ 88.7520 ± 0.0958  │ 65.5120 ± 0.4628  │ 33.9540 ± 0.4242 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam          │ 95.2632 ± 1.6270  │ 85.3333 ± 12.0370 │ 88.8889 ± 3.0429  │ 96.9444 ± 0.2485 │ 95.5360 ± 0.4274  │ 86.6960 ± 0.4723  │ 34.9060 ± 20.3394 │ 16.5000 ± 7.8664 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam_CLARA    │ 95.7895 ± 0.6564  │ 96.0000 ± 1.3333  │ 92.2222 ± 3.2394  │ 98.0000 ± 0.5386 │ 96.8780 ± 0.2227  │ 88.4240 ± 0.1152  │ 47.2060 ± 18.6273 │ 17.9060 ± 8.4849 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam_CLARA_us │ 95.0877 ± 0.7018  │ 96.6667 ± 5.1640  │ 89.4444 ± 3.2394  │ 96.6667 ± 0.6334 │ 98.1680 ± 0.0286  │ 89.1560 ± 0.2060  │ 71.9400 ± 0.5349  │ 35.7380 ± 0.4129 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ D-Adaptation  │ 90.7018 ± 1.1899  │ 96.6667 ± 0.0000  │ 91.6667 ± 4.3033  │ 95.9444 ± 0.3768 │ 43.7060 ± 28.0094 │ 84.2620 ± 0.5259  │ 10.0000 ± 0.0000  │ 1.0000 ± 0.0000  │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│ 1e-01 │ SGD           │ 84.2105 ± 4.6747  │ 62.6667 ± 7.4237  │ 56.1111 ± 11.3039 │ 95.2222 ± 0.2079 │ 98.1560 ± 0.0873  │ 89.7640 ± 0.1194  │ 61.6720 ± 25.8391 │ 40.7540 ± 0.3437 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ SGD_CLARA     │ 93.1579 ± 0.3509  │ 92.6667 ± 7.1181  │ 73.3333 ± 3.3333  │ 96.2778 ± 0.4513 │ 98.2140 ± 0.0758  │ 89.7420 ± 0.0248  │ 74.5300 ± 0.2051  │ 40.4360 ± 0.3097 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ SGD_CLARA_us  │ 90.5263 ± 1.9536  │ 90.0000 ± 6.9921  │ 47.2222 ± 12.0442 │ 96.8333 ± 0.4157 │ 97.8640 ± 0.0831  │ 89.5300 ± 0.2739  │ 71.0140 ± 0.7990  │ 36.1840 ± 0.1816 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam          │ 92.8070 ± 2.5664  │ 100.0000 ± 0.0000 │ 80.0000 ± 9.3624  │ 96.3889 ± 0.7658 │ 10.3700 ± 0.5232  │ 10.0000 ± 0.0000  │ 10.0000 ± 0.0000  │ 1.0000 ± 0.0000  │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam_CLARA    │ 94.7368 ± 1.8400  │ 100.0000 ± 0.0000 │ 91.1111 ± 2.0787  │ 97.0556 ± 0.4513 │ 25.5920 ± 30.1532 │ 67.0340 ± 28.6078 │ 10.0000 ± 0.0000  │ 1.0000 ± 0.0000  │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam_CLARA_us │ 90.3509 ± 1.6644  │ 100.0000 ± 0.0000 │ 89.4444 ± 6.4310  │ 96.7778 ± 0.2833 │ 97.9560 ± 0.0686  │ 88.9180 ± 0.2332  │ 71.7160 ± 0.3205  │ 35.5300 ± 0.6857 │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ D-Adaptation  │ 88.4211 ± 4.0541  │ 96.6667 ± 0.0000  │ 88.3333 ± 6.6667  │ 95.3889 ± 0.5443 │ 10.3740 ± 0.5212  │ 10.0040 ± 0.0080  │ 10.0020 ± 0.0040  │ 1.0000 ± 0.0000  │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│ 1.00  │ SGD           │ 85.6140 ± 2.4561  │ 75.3333 ± 17.2047 │ 60.0000 ± 10.1835 │ 95.3333 ± 0.3239 │ 10.6720 ± 0.5575  │ 12.0960 ± 4.1920  │ 10.0000 ± 0.0000  │ 1.0000 ± 0.0000  │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ SGD_CLARA     │ 93.3333 ± 0.4297  │ 79.3333 ± 9.7525  │ 72.2222 ± 3.9284  │ 96.5000 ± 0.5720 │ 11.3500 ± 0.0000  │ 81.6660 ± 1.6234  │ 10.0000 ± 0.0000  │ 1.0000 ± 0.0000  │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ SGD_CLARA_us  │ 92.1053 ± 0.5548  │ 88.0000 ± 5.8119  │ 70.0000 ± 7.3283  │ 96.4444 ± 0.4082 │ 89.8480 ± 0.5763  │ 87.6880 ± 0.2878  │ 22.7040 ± 15.5600 │ 8.0540 ± 5.8432  │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam          │ 94.0351 ± 1.2892  │ 100.0000 ± 0.0000 │ 78.3333 ± 8.3148  │ 96.2222 ± 0.4157 │ 9.9500 ± 0.9273   │ 10.0020 ± 0.0040  │ 10.0020 ± 0.0040  │ 1.0000 ± 0.0000  │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam_CLARA    │ 93.5088 ± 0.7018  │ 100.0000 ± 0.0000 │ 89.4444 ± 1.1111  │ 96.3889 ± 0.7454 │ 11.3520 ± 0.0040  │ 10.5480 ± 0.9008  │ 10.0020 ± 0.0040  │ 1.0000 ± 0.0000  │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ Adam_CLARA_us │ 95.9649 ± 2.0459  │ 100.0000 ± 0.0000 │ 90.0000 ± 10.3339 │ 96.4444 ± 0.7738 │ 94.7140 ± 0.1059  │ 86.5460 ± 0.4178  │ 10.0020 ± 0.0040  │ 1.0000 ± 0.0000  │\n",
      "├───────┼───────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┼───────────────────┼───────────────────┼───────────────────┼──────────────────┤\n",
      "│       │ D-Adaptation  │ 88.2456 ± 2.2604  │ 96.6667 ± 0.0000  │ 90.5556 ± 5.1520  │ 94.8889 ± 0.1361 │ 9.9500 ± 0.9273   │ 10.0900 ± 0.1413  │ 10.0040 ± 0.0049  │ 1.0000 ± 0.0000  │\n",
      "╘═══════╧═══════════════╧═══════════════════╧═══════════════════╧═══════════════════╧══════════════════╧═══════════════════╧═══════════════════╧═══════════════════╧══════════════════╛\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_algorithm_performance(\n",
    "    summary_stats=summary,  # from your earlier function\n",
    "    algorithms=[\"SGD\", \"SGD_CLARA\", \"SGD_CLARA_us\", \"Adam\", \"Adam_CLARA\", \"Adam_CLARA_us\", \"D-Adaptation\"],\n",
    "    datasets=[\"breast_cancer\", \"iris\", \"wine\", \"digits\", \"mnist\", \"fmnist\", \"cifar10\", \"cifar100\"],\n",
    "    lr_values=[\"1e-06\", \"1e-05\", \"1e-04\", \"1e-03\", \"1e-02\", \"1e-01\", \"1.00\"],\n",
    "    metric_name=\"Test Accuracy\"\n",
    ")"
   ],
   "id": "4998973b9c860ddb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:30:26.695944Z",
     "start_time": "2025-07-18T17:30:26.689254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_algorithm_performance_lines(summary_stats, algorithms, datasets, lr_values, metric_name=\"Test Accuracy\", save_fig=False):\n",
    "    \"\"\"\n",
    "    Plot line charts (instead of bar plots) with error bars for each algorithm at different learning rates,\n",
    "    one plot per dataset. Each algorithm is represented by a different color and marker.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_markers():\n",
    "        return ['o', 's', '^', 'D', 'v', 'P', '>', 'X', 'h', '+']\n",
    "\n",
    "    color_map = get_algorithm_colors(algorithms)\n",
    "    marker_list = get_markers()\n",
    "\n",
    "    for dataset in datasets:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Keeping track of the best test performance across algorithms and learning rate values\n",
    "        global_best = {\"val\": -np.inf, \"lr\": None, \"algo\": None}\n",
    "\n",
    "        for i, algorithm in enumerate(algorithms):\n",
    "            means = []\n",
    "            stds = []\n",
    "\n",
    "            for lr in lr_values:\n",
    "                val = summary_stats.get(lr, {}).get(algorithm, {}).get(dataset)\n",
    "                if val:\n",
    "                    mean, std = val\n",
    "                else:\n",
    "                    mean, std = np.nan, np.nan\n",
    "                means.append(mean)\n",
    "                stds.append(std)\n",
    "\n",
    "            # Convert learning rates to float for plotting\n",
    "            lr_floats = [float(lr) for lr in lr_values]\n",
    "            plt.errorbar(\n",
    "                lr_floats,\n",
    "                means,\n",
    "                yerr=stds,\n",
    "                label=algorithm,\n",
    "                marker=marker_list[i % len(marker_list)],\n",
    "                markersize=8,\n",
    "                color=color_map[algorithm],\n",
    "                capsize=4,\n",
    "                linestyle='-'\n",
    "            )\n",
    "\n",
    "            # Track global best point\n",
    "            max_val = np.nanmax(means)\n",
    "            if max_val > global_best[\"val\"]:\n",
    "                best_idx = np.nanargmax(means)\n",
    "                global_best = {\n",
    "                    \"val\": max_val,\n",
    "                    \"lr\": float(lr_values[best_idx]),\n",
    "                    \"algo\": algorithm\n",
    "                }\n",
    "\n",
    "        # Highlight best overall performance\n",
    "        plt.plot(\n",
    "            global_best[\"lr\"], global_best[\"val\"],\n",
    "            marker='*', color='black', markersize=15,\n",
    "            label=\"Best overall\", zorder=5\n",
    "        )\n",
    "        plt.text(\n",
    "            global_best[\"lr\"], global_best[\"val\"] + 1.5,\n",
    "            f\"{global_best['val']:.2f}\",\n",
    "            ha='center', va='bottom',\n",
    "            fontsize=9, color='black', fontweight='bold'\n",
    "        )\n",
    "\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Initial Learning Rate\")\n",
    "        plt.ylabel(metric_name)\n",
    "        # plt.title(f\"{metric_name} on {dataset}\")\n",
    "        plt.title(f\"{dataset}\")\n",
    "        plt.ylim(0, 100)\n",
    "        plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save as PDF in a subdirectory\n",
    "        if save_fig:\n",
    "            save_dir = \"plots\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            filename = f\"{dataset}_{metric_name.replace(' ', '_')}.pdf\"\n",
    "            filepath = os.path.join(save_dir, filename)\n",
    "            plt.savefig(filepath, format='pdf', bbox_inches='tight')\n",
    "\n",
    "        plt.show()"
   ],
   "id": "97d19721bc900bca",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:30:32.666733Z",
     "start_time": "2025-07-18T17:30:30.876028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plot_algorithm_performance_lines(\n",
    "    summary_stats=summary,\n",
    "    algorithms=[\"SGD\", \"SGD_CLARA\", \"SGD_CLARA_us\", \"Adam\", \"Adam_CLARA\", \"Adam_CLARA_us\", \"D-Adaptation\"],\n",
    "    # algorithms=[\"SGD_CLARA\", \"SGD_CLARA_us\", \"Adam_CLARA\", \"Adam_CLARA_us\"],\n",
    "    datasets=[\"breast_cancer\", \"iris\", \"wine\", \"digits\", \"mnist\", \"fmnist\", \"cifar10\", \"cifar100\"],\n",
    "    lr_values=[\"1e-06\", \"1e-05\", \"1e-04\", \"1e-03\", \"1e-02\", \"1e-01\", \"1.00\"],\n",
    "    metric_name=\"Test Accuracy\",\n",
    "    save_fig=False\n",
    ")"
   ],
   "id": "64974bd63f808ee4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T15:14:55.650312Z",
     "start_time": "2025-07-18T15:14:55.643932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_avg_performance_vs_damping(base_dir, algorithm, datasets, lr_values, metric=\"test_accuracy\"):\n",
    "    \"\"\"\n",
    "    For a given algorithm, plot average test performance across all datasets (with std deviation as error bars)\n",
    "    as a function of damping value. Each line corresponds to a different learning rate.\n",
    "    \"\"\"\n",
    "    # Pattern: date_time_epoch_seed_damping\n",
    "    # folder_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2})_(\\d{2}-\\d{2})_(\\d+)_([\\d]+)_([0-9eE\\.-]+)\")\n",
    "    folder_pattern = re.compile(r\"(\\d+)_([\\d]+)_([0-9eE\\.-]+)_EpochSeedDamping\")\n",
    "\n",
    "    # results[damping][lr] = list of metric values (avg over datasets and seeds)\n",
    "    results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for entry in os.listdir(base_dir):\n",
    "        match = folder_pattern.match(entry)\n",
    "        if not match:\n",
    "            continue\n",
    "        _, _, damping = match.groups()\n",
    "        damping = float(damping)\n",
    "        # print(\"Damping: \", damping)\n",
    "        run_dir = os.path.join(base_dir, entry)\n",
    "\n",
    "        for dataset in datasets:\n",
    "            dataset_path = os.path.join(run_dir, dataset)\n",
    "            if not os.path.isdir(dataset_path):\n",
    "                continue\n",
    "\n",
    "            for lr in lr_values:\n",
    "                lr_path = os.path.join(dataset_path, f\"lr{lr}\")\n",
    "                result_file = os.path.join(lr_path, f\"{algorithm}.pkl\")\n",
    "                if os.path.isfile(result_file):\n",
    "                    with open(result_file, \"rb\") as f:\n",
    "                        data = pickle.load(f)\n",
    "                        value = data.get(metric)\n",
    "                        if value is not None:\n",
    "                            results[damping][lr].append(value)\n",
    "\n",
    "    # Sort damping values\n",
    "    damping_values = sorted(results.keys())\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for lr in lr_values:\n",
    "        means = []\n",
    "        stds = []\n",
    "        for damping in damping_values:\n",
    "            vals = results[damping].get(lr, [])\n",
    "            if vals:\n",
    "                means.append(np.mean(vals))\n",
    "                stds.append(np.std(vals))\n",
    "            else:\n",
    "                means.append(np.nan)\n",
    "                stds.append(np.nan)\n",
    "\n",
    "        plt.errorbar(damping_values, means, yerr=stds, marker='o', label=f\"lr={lr}\", capsize=4)\n",
    "\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Damping\")\n",
    "    plt.ylabel(metric.replace(\"_\", \" \").capitalize())\n",
    "    plt.title(f\"Performance over all datasets for {algorithm}\")\n",
    "    plt.ylim(0, 100)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    save_dir = \"plots_vs_damping\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filename = f\"{algorithm}_{metric}.pdf\"\n",
    "    filepath = os.path.join(save_dir, filename)\n",
    "    plt.savefig(filepath, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()"
   ],
   "id": "f1bb79b685ff29ac",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T15:15:49.388089Z",
     "start_time": "2025-07-18T15:15:48.748603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plot_avg_performance_vs_damping(\n",
    "    base_dir=\"TM_Experiments/selected_results/damping_experiments/\",\n",
    "    algorithm=\"SGD_CLARA_us\",\n",
    "    datasets=[\"breast_cancer\", \"iris\", \"wine\", \"digits\", \"mnist\", \"fmnist\", \"cifar10\", \"cifar100\"],\n",
    "    lr_values=[\"1e-06\", \"1e-05\", \"1e-04\", \"1e-03\", \"1e-02\", \"1e-01\", \"1.00\"],\n",
    "    metric=\"test_accuracy\"\n",
    ")\n",
    "\n",
    "# algorithms=[\"SGD\", \"SGD_CLARA\", \"SGD_CLARA_us\", \"Adam\", \"Adam_CLARA\", \"Adam_CLARA_us\", \"D-Adaptation\"]"
   ],
   "id": "c378859e6cf97783",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:50:46.972989Z",
     "start_time": "2025-07-18T17:50:46.964423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_performance_vs_damping_per_dataset(base_dir, algorithm, datasets, lr_values, metric=\"test_accuracy\"):\n",
    "    \"\"\"\n",
    "    For a given algorithm, plot test performance as a function of damping (with error bars) for each dataset\n",
    "    and each learning rate. Also plots best damping as a function of learning rate.\n",
    "    \"\"\"\n",
    "    folder_pattern = re.compile(r\"(\\d+)_([\\d]+)_([0-9eE\\.-]+)_EpochSeedDamping\")\n",
    "\n",
    "    for dataset in datasets:\n",
    "        results = defaultdict(lambda: defaultdict(list))  # results[damping][lr] = list of metric values\n",
    "\n",
    "        for entry in os.listdir(base_dir):\n",
    "            match = folder_pattern.match(entry)\n",
    "            if not match:\n",
    "                continue\n",
    "            _, _, damping = match.groups()\n",
    "            damping = float(damping)\n",
    "            run_dir = os.path.join(base_dir, entry)\n",
    "\n",
    "            dataset_path = os.path.join(run_dir, dataset)\n",
    "            if not os.path.isdir(dataset_path):\n",
    "                continue\n",
    "\n",
    "            for lr in lr_values:\n",
    "                lr_path = os.path.join(dataset_path, f\"lr{lr}\")\n",
    "                result_file = os.path.join(lr_path, f\"{algorithm}.pkl\")\n",
    "                if os.path.isfile(result_file):\n",
    "                    with open(result_file, \"rb\") as f:\n",
    "                        data = pickle.load(f)\n",
    "                        value = data.get(metric)\n",
    "                        if value is not None:\n",
    "                            results[damping][lr].append(value)\n",
    "\n",
    "        damping_values = sorted(results.keys())\n",
    "        best_dampings = []\n",
    "\n",
    "        # Plot: performance vs. damping (1 plot per dataset)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for lr in lr_values:\n",
    "            means, stds = [], []\n",
    "            for damping in damping_values:\n",
    "                vals = results[damping].get(lr, [])\n",
    "                if vals:\n",
    "                    means.append(np.mean(vals))\n",
    "                    stds.append(np.std(vals))\n",
    "                else:\n",
    "                    means.append(np.nan)\n",
    "                    stds.append(np.nan)\n",
    "\n",
    "            # Get best damping for this lr\n",
    "            valid = [(d, m) for d, m in zip(damping_values, means) if not np.isnan(m)]\n",
    "            if valid:\n",
    "                best_damping, best_value = max(valid, key=lambda x: x[1])\n",
    "                best_dampings.append((float(lr), best_damping))\n",
    "\n",
    "                # Copy best model to target folder\n",
    "                for s in range(5):\n",
    "                    source_path = f\"TM_Experiments/selected_results/damping_experiments/100_{s}_{best_damping:.0e}_EpochSeedDamping/{dataset}/lr{lr}/{algorithm}.pkl\"\n",
    "                    dest_path = f\"TM_Experiments/selected_results/100_{s}_EpochSeed/{dataset}/lr{lr}/\"\n",
    "                    os.makedirs(dest_path, exist_ok=True)\n",
    "                    if os.path.exists(source_path):\n",
    "                        shutil.copy(source_path, dest_path)\n",
    "\n",
    "            plt.errorbar(\n",
    "                damping_values, means, yerr=stds,\n",
    "                marker='o', label=f\"lr={lr}\", capsize=5\n",
    "            )\n",
    "\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Damping\")\n",
    "        plt.ylabel(metric.replace(\"_\", \" \").title())\n",
    "        plt.title(f\"{algorithm} on {dataset}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.ylim(0, 100)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save performance vs damping plot\n",
    "        save_dir = \"plots_vs_damping\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        filename = f\"{dataset}_{algorithm}_{metric}.pdf\"\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "        plt.savefig(filepath, format=\"pdf\", bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "        # Plot: best damping vs. learning rate\n",
    "        if best_dampings:\n",
    "            lr_floats, best_damp_vals = zip(*sorted(best_dampings))\n",
    "\n",
    "            plt.figure(figsize=(7, 5))\n",
    "            plt.plot(lr_floats, best_damp_vals, marker='o', linestyle='-')\n",
    "            plt.xscale(\"log\")\n",
    "            plt.yscale(\"log\")\n",
    "            plt.yscale(\"log\")\n",
    "            plt.ylim(1e-5, 1e-1)\n",
    "            plt.xlabel(\"Initial Learning Rate\")\n",
    "            plt.ylabel(\"Best Damping\")\n",
    "            plt.title(f\"Best Damping vs Learning Rate\\n{algorithm} on {dataset}\")\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save best damping vs learning rate plot\n",
    "            best_plot_dir = \"plots_best_damping\"\n",
    "            os.makedirs(best_plot_dir, exist_ok=True)\n",
    "            best_filename = f\"{dataset}_{algorithm}_{metric}_best_damping_vs_lr.pdf\"\n",
    "            best_filepath = os.path.join(best_plot_dir, best_filename)\n",
    "            plt.savefig(best_filepath, format=\"pdf\", bbox_inches=\"tight\")\n",
    "            plt.close()"
   ],
   "id": "25f57edd9b1e6954",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:51:37.242119Z",
     "start_time": "2025-07-18T17:51:31.001271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plot_performance_vs_damping_per_dataset(\n",
    "    base_dir=\"TM_Experiments/selected_results/damping_experiments/\",\n",
    "    algorithm=\"SGD_CLARA_us\",\n",
    "    datasets=[\"breast_cancer\", \"iris\", \"wine\", \"digits\", \"mnist\", \"fmnist\", \"cifar10\", \"cifar100\"],\n",
    "    lr_values=[\"1e-06\", \"1e-05\", \"1e-04\", \"1e-03\", \"1e-02\", \"1e-01\", \"1.00\"],\n",
    "    metric=\"test_accuracy\"\n",
    ")"
   ],
   "id": "1da857c8550930e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T15:00:28.534085Z",
     "start_time": "2025-07-21T15:00:28.526198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_avg_lr_and_accuracy_schedules(base_dir_template, seeds, dataset, algorithms):\n",
    "    \"\"\"\n",
    "    For each algorithm, plot average learning rate and training accuracy schedules across seeds,\n",
    "    grouped by initial learning rate. Plots appear in two stacked subplots with a shared legend.\n",
    "    \"\"\"\n",
    "    for algorithm in algorithms:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), sharey=False)\n",
    "\n",
    "        example_dir = base_dir_template.format(seeds[0])\n",
    "        dataset_path = os.path.join(example_dir, dataset)\n",
    "        lr_folders = [f for f in os.listdir(dataset_path) if f.startswith(\"lr\")]\n",
    "\n",
    "        lr_entries = []\n",
    "        for folder in lr_folders:\n",
    "            try:\n",
    "                lr_str = folder[2:]\n",
    "                lr_float = float(lr_str)\n",
    "                lr_entries.append((lr_str, lr_float))\n",
    "            except ValueError:\n",
    "                continue\n",
    "        lr_entries.sort(key=lambda x: x[1])\n",
    "\n",
    "        for lr_str, _ in lr_entries:\n",
    "            lr_histories = []\n",
    "            acc_histories = []\n",
    "\n",
    "            for seed in seeds:\n",
    "                base_dir = base_dir_template.format(seed)\n",
    "                pkl_path = os.path.join(base_dir, dataset, f\"lr{lr_str}\", f\"{algorithm}.pkl\")\n",
    "                if os.path.isfile(pkl_path):\n",
    "                    with open(pkl_path, \"rb\") as f:\n",
    "                        data = pickle.load(f)\n",
    "                        lr_history = data.get(\"lr_history\")\n",
    "\n",
    "                        # contains_nan = np.isnan(lr_history).any()\n",
    "                        # if contains_nan:\n",
    "                        #     print(lr_history)\n",
    "\n",
    "                        acc_history = data.get(\"train_accuracies\")\n",
    "                        if lr_history is not None and acc_history is not None:\n",
    "                            lr_histories.append(lr_history)\n",
    "                            acc_histories.append(acc_history)\n",
    "\n",
    "            if not lr_histories:\n",
    "                continue\n",
    "\n",
    "            # Compute mean and std\n",
    "            lr_array = np.array(lr_histories)\n",
    "            acc_array = np.array(acc_histories)\n",
    "            mean_lr = np.mean(lr_array, axis=0)\n",
    "            std_lr = np.std(lr_array, axis=0)\n",
    "            mean_acc = np.mean(acc_array, axis=0)\n",
    "            std_acc = np.std(acc_array, axis=0)\n",
    "            steps = np.arange(len(mean_lr))\n",
    "\n",
    "            label = f\"lr={lr_str}\"\n",
    "            ax2.plot(steps, mean_lr, label=label)\n",
    "            ax2.fill_between(steps, mean_lr - std_lr, mean_lr + std_lr, alpha=0.2)\n",
    "\n",
    "            ax1.plot(steps, mean_acc, label=label)\n",
    "            ax1.fill_between(steps, mean_acc - std_acc, mean_acc + std_acc, alpha=0.2)\n",
    "\n",
    "        plt.suptitle(f\"{algorithm} on {dataset}\")\n",
    "\n",
    "        # Format bottom plot (learning rate)\n",
    "        ax2.set_xlabel(\"Epoch\")\n",
    "        ax2.set_yscale(\"log\")\n",
    "        ax2.set_ylabel(\"Learning Rate\")\n",
    "        ax2.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        # Format top plot (accuracy)\n",
    "        ax1.set_xlabel(\"Epoch\")\n",
    "        ax1.set_ylabel(\"Train Accuracy\")\n",
    "        ax1.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "        ax1.set_ylim(0, 100)\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save best damping vs learning rate plot\n",
    "        lr_plot_dir = \"learning_curves\"\n",
    "        os.makedirs(lr_plot_dir, exist_ok=True)\n",
    "        lr_filename = f\"{dataset}_{algorithm}_training_acc_lr.pdf\"\n",
    "        lr_filepath = os.path.join(lr_plot_dir, lr_filename)\n",
    "        plt.savefig(lr_filepath, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "        plt.show()\n"
   ],
   "id": "32100cf3b71a16c2",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T15:05:15.970428Z",
     "start_time": "2025-07-21T15:05:13.317896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plot_avg_lr_and_accuracy_schedules(base_dir_template=\"TM_Experiments/selected_results/100_{}_EpochSeed\",  # \"TM_Experiments/selected_results/damping_experiments/100_{}_1e-03_EpochSeedDamping\",\n",
    "                                   seeds=[0, 1, 2, 3, 4],\n",
    "                                   dataset=\"cifar100\",\n",
    "                                   algorithms=[\"SGD\", \"SGD_CLARA\", \"SGD_CLARA_us\", \"Adam\", \"Adam_CLARA\", \"Adam_CLARA_us\", \"D-Adaptation\"]\n",
    "                                   )\n",
    "\n",
    "# datasets=[\"breast_cancer\", \"iris\", \"wine\", \"digits\", \"mnist\", \"fmnist\", \"cifar10\", \"cifar100\"]"
   ],
   "id": "7a0a044d30677ec",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fc0032d07df7d19"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
