{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# Define the Sphere function\n",
    "def sphere(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "# Gradient of the Sphere function (known)\n",
    "def sphere_gradient(x):\n",
    "    # noise = np.random.randn(*x.shape)\n",
    "    return 2 * x # + noise * np.linalg.norm(x)\n",
    "\n",
    "cond = 1e6\n",
    "def ellipsoid(x, cond=cond):\n",
    "    return sum(cond**(np.arange(len(x)) / (len(x) - 1 + 1e-9)) * np.asarray(x)**2)\n",
    "\n",
    "def ellipsoid_gradient(x, cond=cond):\n",
    "    return 2 * cond**(np.arange(len(x)) / (len(x) - 1 + 1e-9)) * np.asarray(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def evolution_strategy(f, x0, sigma, iterations=100, mu=5, lambda_=10):\n",
    "    dim = len(x0)\n",
    "    x = x0\n",
    "\n",
    "    # Initialize path variable, decay and damping factors for Cumulative LeArning Rate Adaptation (CLARA)\n",
    "    path = np.zeros(dim)\n",
    "    c = 0.2\n",
    "    d = 0.2\n",
    "    cos_theta = 0\n",
    "\n",
    "    # Initialize return variables\n",
    "    candidate_solutions = []\n",
    "    step_size = []\n",
    "    path_norm = []\n",
    "    step_norm = []\n",
    "    cos_thetas = []\n",
    "\n",
    "    weights = np.linspace(1, 2, mu)  # Assign higher weight to best individuals\n",
    "    weights /= weights.sum()  # Normalize weights to sum to 1\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Generate offspring\n",
    "        pop = np.random.randn(lambda_, dim)\n",
    "\n",
    "        # Select mu best individuals\n",
    "        selected = pop[np.argsort([f(x + sigma * ind) for ind in pop])[:mu]]\n",
    "\n",
    "        # Update current solution\n",
    "        step = np.sum(selected.T * weights, axis=1)\n",
    "        x = x + sigma * step\n",
    "        candidate_solutions.append(x)\n",
    "\n",
    "        # Update mean estimate of cosine of the angle change between current and mean gradient\n",
    "        cos_theta = (1 - c) * cos_theta + np.sqrt(c * (2 - c)) * np.dot(step, path) / (np.linalg.norm(path) * np.linalg.norm(step) + 1e-8)\n",
    "        cos_thetas.append(cos_theta)\n",
    "\n",
    "        # Update path\n",
    "        path = (1 - c) * path + np.sqrt(c * (2 - c)) * step\n",
    "        path_norm.append(np.linalg.norm(path)**2)\n",
    "        step_norm.append(np.linalg.norm(step)**2)\n",
    "\n",
    "        # Update step-size\n",
    "        sigma = sigma * np.exp(d * ((np.linalg.norm(path)**2 / dim) - 1))\n",
    "        step_size.append(sigma)\n",
    "\n",
    "\n",
    "        print(f'Iteration {i}: current fitness = {f(x)}')\n",
    "\n",
    "    return candidate_solutions, step_size, path_norm, step_norm, cos_thetas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "def gradient_descent(gradient, x0, lr=0.1, iterations=100):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to minimize a function.\n",
    "\n",
    "    :param gradient: Function that computes the gradient âˆ‡f(x).\n",
    "    :param x0: Initial guess (NumPy array).\n",
    "    :param lr: Learning rate (step size).\n",
    "    :param iterations: Number of iterations.\n",
    "    :return: Final optimized value of x.\n",
    "    \"\"\"\n",
    "    x = x0  # Initialize x\n",
    "    dim = len(x0)  # Get dimension of search space\n",
    "\n",
    "    # Initialize path variable, decay and damping factors for Cumulative LeArning Rate Adaptation (CLARA)\n",
    "    path = np.zeros(dim)\n",
    "    v = np.zeros(dim)\n",
    "    cos_theta = 0\n",
    "    c = 0.2\n",
    "    d = 1e-4\n",
    "\n",
    "    # Estimate of average of cos(angle) between 2 Gaussian vectors\n",
    "    # est_angle = average_angle_gaussian_vectors(dim)\n",
    "\n",
    "    # Initialize return variables\n",
    "    candidate_solutions = []\n",
    "    learning_rate = []\n",
    "    path_norm = []\n",
    "    gradient_norm = []\n",
    "    cos_thetas = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        grad = gradient(x)  # Compute gradient\n",
    "        x = x - lr * grad  # Update step\n",
    "\n",
    "        # Update mean estimate of cosine of the angle change between current and mean gradient\n",
    "        cos_theta = (1 - c) * cos_theta + np.sqrt(c * (2 - c)) * np.dot(grad, path) / (np.linalg.norm(path) * np.linalg.norm(grad) + 1e-8)\n",
    "\n",
    "        # Update path\n",
    "        path = (1 - c) * path + np.sqrt(c * (2 - c)) * grad\n",
    "        v = (1 - c) * v + np.sqrt(c * (2 - c)) * grad**2\n",
    "\n",
    "        # Update learning rate\n",
    "        # lr = lr * np.exp(d * (np.linalg.norm(path)**2 / (dim + np.linalg.norm(grad)**2) - 1))\n",
    "        lr = lr * np.exp(d * (np.linalg.norm(path)**2 / dim - 1))\n",
    "\n",
    "        candidate_solutions.append(x)\n",
    "        learning_rate.append(lr)\n",
    "        path_norm.append(np.linalg.norm(path)**2)\n",
    "        gradient_norm.append(np.linalg.norm(grad)**2)\n",
    "        cos_thetas.append(cos_theta)\n",
    "\n",
    "    print('Optimized x: ', x)\n",
    "\n",
    "    return candidate_solutions, learning_rate, path_norm, gradient_norm, cos_thetas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x:  [3.0824503e-10 3.0824503e-10]\n"
     ]
    }
   ],
   "source": [
    "dim = 2\n",
    "x0 = np.ones(2)\n",
    "lr0 = 1e0  # 1e-3\n",
    "budget = 500\n",
    "\n",
    "choice = 0\n",
    "\n",
    "if choice == 0:\n",
    "    optimizer = gradient_descent  # evolution_strategy\n",
    "    f = sphere_gradient\n",
    "    fig_title = 'Vanilla gradient descent'  # '(mu, lambda)-ES'\n",
    "else:\n",
    "    optimizer = evolution_strategy\n",
    "    f = sphere\n",
    "    fig_title = '(mu, lambda)-ES'\n",
    "\n",
    "candidate_sol, learning_rates, path_norm, grad_norm, cos_theta = optimizer(f, x0, lr0, iterations=budget)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "results = [[np.linalg.norm(x)**2 for x in candidate_sol],\n",
    "           learning_rates,\n",
    "           path_norm,\n",
    "           grad_norm,\n",
    "           [path_norm[i] / grad_norm[i] for i in range(len(path_norm))],  # grad_norm[i]\n",
    "           cos_theta\n",
    "           ]\n",
    "fig_titles = ['Distance to optimum',\n",
    "              'Learning rate',\n",
    "              'Path norm',\n",
    "              'Gradient/step norm',\n",
    "              'Path-gradient/step ratio',\n",
    "              'Angle between path and current grad./step'\n",
    "              ]\n",
    "y_labels = [r'$\\|x_t\\|^2$',\n",
    "            r'$\\alpha_t$',\n",
    "            r'$\\|p_t\\|^2$',\n",
    "            r'$\\|g_t\\|^2$',\n",
    "            r'$\\|p_t\\|^2 / \\|g_t\\|^2$',\n",
    "            r'$\\cos(\\theta)$'\n",
    "            ]\n",
    "colors = ['r',\n",
    "          'g',\n",
    "          'b',\n",
    "          'c',\n",
    "          'm',\n",
    "          'y'\n",
    "          ]\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 12))  # 2 rows, 2 columns\n",
    "fig.suptitle(f'{fig_title} on the sphere, D = {str(len(x0))}')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):  # Iterate over subplots\n",
    "    if i == len(results) - 1:\n",
    "        ax.plot(results[i], color=colors[i])\n",
    "    else:\n",
    "        ax.semilogy(results[i], color=colors[i])\n",
    "    ax.set_title(fig_titles[i])\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel(y_labels[i])\n",
    "    ax.grid(True)\n",
    "\n",
    "# Adjust layout and show\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
