{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Define the Sphere function\n",
    "def sphere(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "# Gradient of the Sphere function (known)\n",
    "def grad_sphere(x):\n",
    "    return 2 * x\n",
    "\n",
    "def noisy_sphere(x):\n",
    "    noise = np.random.normal(loc=0, scale=0.01)\n",
    "    return np.sum(x**2) * np.exp(noise)\n",
    "\n",
    "# Gradient of the Sphere function (known)\n",
    "def noisy_grad_sphere(x):\n",
    "    noise = np.random.randn(*x.shape) * 0.5\n",
    "    return 2 * (1 + noise) * x  # + noise * np.linalg.norm(x)\n",
    "\n",
    "cond = 1e6\n",
    "def ellipsoid(x, cond=cond):\n",
    "    return sum(cond**(np.arange(len(x)) / (len(x) - 1 + 1e-9)) * np.asarray(x)**2)\n",
    "\n",
    "def grad_ellipsoid(x, cond=cond):\n",
    "    noise = np.random.randn(*x.shape)\n",
    "    return 2 * cond**(np.arange(len(x)) / (len(x) - 1 + 1e-9)) * np.asarray(x) + noise * np.linalg.norm(x)\n",
    "\n",
    "def noisy_ellipsoid(x, cond=cond):\n",
    "    noise = np.random.normal(loc=0, scale=0.01)\n",
    "    return sum(cond**(np.arange(len(x)) / (len(x) - 1 + 1e-9)) * np.asarray(x)**2) * np.exp(noise)\n",
    "\n",
    "def noisy_grad_ellipsoid(x, cond=cond):\n",
    "    noise = np.random.randn(*x.shape) * 0.5\n",
    "    return 2 * (1 + noise) * cond**(np.arange(len(x)) / (len(x) - 1 + 1e-9)) * np.asarray(x)  # + noise * np.linalg.norm(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def evolution_strategy(f, x0, sigma, iterations=100, mu=5, lambda_=10):\n",
    "    dim = len(x0)\n",
    "    x = x0\n",
    "\n",
    "    # Initialize path variable, smoothing and damping factors for Cumulative LeArning Rate Adaptation (CLARA)\n",
    "    path = np.zeros(dim)\n",
    "    c = 0.2\n",
    "    d = 1\n",
    "\n",
    "    # Initialize return variables\n",
    "    candidate_solutions = []\n",
    "    step_size = []\n",
    "    path_norm = []\n",
    "    step_norm = []\n",
    "\n",
    "    weights = np.linspace(1, 2, mu)  # Assign higher weight to best individuals\n",
    "    weights /= weights.sum()  # Normalize weights to sum to 1\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Generate offspring\n",
    "        pop = np.random.randn(lambda_, dim)\n",
    "\n",
    "        # Select mu best individuals\n",
    "        selected = pop[np.argsort([f(x + sigma * ind) for ind in pop])[:mu]]\n",
    "\n",
    "        # Update current solution\n",
    "        step = np.sum(selected.T * weights, axis=1)\n",
    "        x = x + sigma * step\n",
    "        candidate_solutions.append(x)\n",
    "\n",
    "        # Update path\n",
    "        path = (1 - c) * path + np.sqrt(c * (2 - c)) * step\n",
    "        path_norm.append(np.linalg.norm(path))\n",
    "        step_norm.append(np.linalg.norm(step))\n",
    "\n",
    "        # Update step-size\n",
    "        sigma = sigma * np.exp(c / (2 * d) * ((np.linalg.norm(path)**2 / dim) - 1))\n",
    "        step_size.append(sigma)\n",
    "\n",
    "\n",
    "        print(f'Iteration {i}: current fitness = {f(x)}')\n",
    "\n",
    "    return candidate_solutions, step_size, path_norm, step_norm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def gradient_descent(gradient, x0, lr=0.1, iterations=100, adapt_lr=True):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to minimize a function.\n",
    "\n",
    "    :param gradient: Function that computes the gradient ∇f(x).\n",
    "    :param x0: Initial guess (NumPy array).\n",
    "    :param lr: Learning rate (step size).\n",
    "    :param iterations: Number of iterations.\n",
    "    :return: Final optimized value of x.\n",
    "    \"\"\"\n",
    "    x = x0  # Initialize x\n",
    "    dim = len(x0)  # Get dimension of search space\n",
    "\n",
    "    # Initialize path variable, smoothing and damping factors for Cumulative LeArning Rate Adaptation (CLARA)\n",
    "    path = np.zeros(dim)\n",
    "    c = 0.2\n",
    "    d = 1\n",
    "\n",
    "    # Initialize return variables\n",
    "    candidate_solutions = []\n",
    "    learning_rate = []\n",
    "    path_norm = []\n",
    "    gradient_norm = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        grad = gradient(x)  # Compute gradient\n",
    "        step = grad.copy()\n",
    "        if adapt_lr:\n",
    "            step /= np.linalg.norm(step)  # TODO: Handle division by zero\n",
    "        x = x - lr * step # Update step\n",
    "\n",
    "        # Update path\n",
    "        path = (1 - c) * path + np.sqrt(c * (2 - c)) * step\n",
    "\n",
    "        # Update learning rate\n",
    "        if adapt_lr:\n",
    "            lr = lr * np.exp(c / (2 * d) * (np.linalg.norm(path)**2 / dim - 1))\n",
    "\n",
    "        candidate_solutions.append(x)\n",
    "        learning_rate.append(lr)\n",
    "        path_norm.append(np.linalg.norm(path))\n",
    "        gradient_norm.append(np.linalg.norm(grad))\n",
    "\n",
    "    print('Optimized x: ', x)\n",
    "\n",
    "    return candidate_solutions, learning_rate, path_norm, gradient_norm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def adam(gradient, x0, lr=0.1, iterations=100, adapt_lr=True):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to minimize a function.\n",
    "\n",
    "    :param gradient: Function that computes the gradient ∇f(x).\n",
    "    :param x0: Initial guess (NumPy array).\n",
    "    :param lr: Learning rate (step size).\n",
    "    :param iterations: Number of iterations.\n",
    "    :return: Final optimized value of x.\n",
    "    \"\"\"\n",
    "    x = x0  # Initialize x\n",
    "    dim = len(x0)  # Get dimension of search space\n",
    "\n",
    "    # Initialize path variable, smoothing and damping factors for Cumulative LeArning Rate Adaptation (CLARA)\n",
    "    path = np.zeros(dim)\n",
    "    c = 0.2\n",
    "    d = 1\n",
    "\n",
    "    # Initialize Adam variables for calculating first and second moments\n",
    "    m = np.zeros(dim)\n",
    "    v = np.zeros(dim)\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "\n",
    "    # Initialize return variables\n",
    "    candidate_solutions = []\n",
    "    learning_rate = []\n",
    "    path_norm = []\n",
    "    gradient_norm = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        grad = gradient(x)  # Compute gradient\n",
    "\n",
    "        # Update step\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        m_hat = m / (1 - beta1**(i + 1))\n",
    "        v = beta2 * v + (1 - beta2) * grad**2\n",
    "        v_hat = v / (1 - beta2**(i + 1))\n",
    "        step_adam = m_hat / (np.sqrt(v_hat) + 1e-8)\n",
    "        if adapt_lr:\n",
    "            step_adam /= np.linalg.norm(step_adam)  # TODO: Handle division by zero\n",
    "        # Update solution\n",
    "        x = x - lr * step_adam\n",
    "\n",
    "        # Update path of steps taken\n",
    "        path = (1 - c) * path + np.sqrt(c * (2 - c)) * step_adam\n",
    "\n",
    "        # Update learning rate\n",
    "        if adapt_lr:\n",
    "            lr = lr * np.exp(c / (2 * d) * (np.linalg.norm(path)**2 / dim - 1))\n",
    "\n",
    "        candidate_solutions.append(x)\n",
    "        learning_rate.append(lr)\n",
    "        path_norm.append(np.linalg.norm(path))\n",
    "        gradient_norm.append(np.linalg.norm(grad))\n",
    "\n",
    "    print('Optimized x: ', x)\n",
    "\n",
    "    return candidate_solutions, learning_rate, path_norm, gradient_norm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x:  [-6.95553490e-13 -1.85363542e-12]\n"
     ]
    }
   ],
   "source": [
    "dim = 2\n",
    "x0 = np.ones(dim)\n",
    "lr0 = 1e6\n",
    "budget = 1000\n",
    "adapt_lr = True\n",
    "\n",
    "choice = 1\n",
    "\n",
    "f = noisy_grad_ellipsoid\n",
    "# f_name = r'$f_\\text{sphere}$'\n",
    "# f_name = r'$f_\\text{elli}$'\n",
    "# f_name = r'$\\tilde{f}_\\text{sphere}$'\n",
    "f_name = r'$\\tilde{f}_\\text{elli}$'\n",
    "\n",
    "if choice == 0:  # Vanilla gradient descent\n",
    "    optimizer = gradient_descent\n",
    "    fig_title = 'Gradient descent'  # '(mu, lambda)-ES'\n",
    "    fig_name = 'gd_'\n",
    "elif choice == 1:  # Adam\n",
    "    optimizer = adam\n",
    "    fig_title = 'Adam'\n",
    "    fig_name = 'adam_'\n",
    "else:  # Evolution strategy\n",
    "    optimizer = evolution_strategy\n",
    "    f = sphere\n",
    "    fig_title = r'($\\mu, \\lambda$)-ES'\n",
    "    fig_name = 'es_'\n",
    "\n",
    "if adapt_lr:\n",
    "    fig_title += ' with CLARA'\n",
    "    fig_name += 'clara_'\n",
    "\n",
    "if f == grad_sphere or f == sphere:\n",
    "    fig_name += 'sphere_'\n",
    "elif f == grad_ellipsoid or f == ellipsoid:\n",
    "    fig_name += 'elli_'\n",
    "elif f == noisy_grad_sphere or f == noisy_sphere:\n",
    "    fig_name += 'sphere_noisy_'\n",
    "elif f == noisy_grad_ellipsoid or f == noisy_ellipsoid:\n",
    "    fig_name += 'elli_noisy_'\n",
    "\n",
    "fig_name += 'lr0_' + str(lr0)\n",
    "\n",
    "if choice == 0 or choice == 1:\n",
    "    candidate_sol, learning_rates, path_norm, grad_norm = optimizer(f, x0, lr0, iterations=budget, adapt_lr=adapt_lr)\n",
    "else:\n",
    "    candidate_sol, learning_rates, path_norm, grad_norm= optimizer(f, x0, lr0, iterations=budget)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "results = [[np.linalg.norm(x) for x in candidate_sol],\n",
    "           learning_rates,\n",
    "           path_norm,\n",
    "           grad_norm,\n",
    "           [path_norm[i]**2 / dim for i in range(len(path_norm))]\n",
    "           ]\n",
    "fig_titles = ['Distance to optimum',\n",
    "              'Learning rate',\n",
    "              'Path norm',\n",
    "              'Gradient norm',\n",
    "              'Normalized path norm'\n",
    "              ]\n",
    "y_labels = [r'$\\|x_t\\|$',\n",
    "            r'$\\eta_t$',\n",
    "            r'$\\|p_t\\|$',\n",
    "            r'$\\|g_t\\|$',\n",
    "            r'$\\|p_t\\|^2 / n$'\n",
    "            ]\n",
    "colors = ['r',\n",
    "          'g',\n",
    "          'b',\n",
    "          'c',\n",
    "          'm'\n",
    "          ]\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(6, 12))  # 2 rows, 2 columns\n",
    "fig.suptitle(rf'{fig_title} on {f_name}, $n$ = {str(len(x0))}, $\\eta_0$ = {lr0:.0e}')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):  # Iterate over subplots\n",
    "    if i < len(results):\n",
    "        ax.semilogy(results[i], color=colors[i])\n",
    "        ax.set_title(fig_titles[i])\n",
    "        ax.set_xlabel('Iterations')\n",
    "        ax.set_ylabel(y_labels[i])\n",
    "        ax.grid(True)\n",
    "\n",
    "# Adjust layout and show\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_name + '.pdf')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
