{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Define the Sphere function\n",
    "def sphere(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "# Gradient of the Sphere function (known)\n",
    "def sphere_gradient(x):\n",
    "    noise = np.random.randn(*x.shape)\n",
    "    return 2 * x + noise * np.linalg.norm(x)\n",
    "\n",
    "cond = 1e3\n",
    "def ellipsoid(x, cond=cond):\n",
    "    return sum(cond**(np.arange(len(x)) / (len(x) - 1 + 1e-9)) * np.asarray(x)**2)\n",
    "\n",
    "def ellipsoid_gradient(x, cond=cond):\n",
    "    return 2 * cond**(np.arange(len(x)) / (len(x) - 1 + 1e-9)) * np.asarray(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def evolution_strategy(f, x0, sigma, iterations=100, mu=5, lambda_=10):\n",
    "    dim = len(x0)\n",
    "    x = x0\n",
    "\n",
    "    # Initialize path variable, decay and damping factors for Cumulative LeArning Rate Adaptation (CLARA)\n",
    "    path = np.zeros(dim)\n",
    "    c = 0.2\n",
    "    d = 0.2\n",
    "    cos_theta = 0\n",
    "\n",
    "    # Initialize return variables\n",
    "    candidate_solutions = []\n",
    "    step_size = []\n",
    "    path_norm = []\n",
    "    step_norm = []\n",
    "    cos_thetas = []\n",
    "\n",
    "    weights = np.linspace(1, 2, mu)  # Assign higher weight to best individuals\n",
    "    weights /= weights.sum()  # Normalize weights to sum to 1\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Generate offspring\n",
    "        pop = np.random.randn(lambda_, dim)\n",
    "\n",
    "        # Select mu best individuals\n",
    "        selected = pop[np.argsort([f(x + sigma * ind) for ind in pop])[:mu]]\n",
    "\n",
    "        # Update current solution\n",
    "        step = np.sum(selected.T * weights, axis=1)\n",
    "        x = x + sigma * step\n",
    "        candidate_solutions.append(x)\n",
    "\n",
    "        # Update mean estimate of cosine of the angle change between current and mean gradient\n",
    "        cos_theta = (1 - c) * cos_theta + np.sqrt(c * (2 - c)) * np.dot(step, path) / (np.linalg.norm(path) * np.linalg.norm(step) + 1e-8)\n",
    "        cos_thetas.append(cos_theta)\n",
    "\n",
    "        # Update path\n",
    "        path = (1 - c) * path + np.sqrt(c * (2 - c)) * step\n",
    "        path_norm.append(np.linalg.norm(path)**2)\n",
    "        step_norm.append(np.linalg.norm(step)**2)\n",
    "\n",
    "        # Update step-size\n",
    "        sigma = sigma * np.exp(d * ((np.linalg.norm(path)**2 / dim) - 1))\n",
    "        step_size.append(sigma)\n",
    "\n",
    "\n",
    "        print(f'Iteration {i}: current fitness = {f(x)}')\n",
    "\n",
    "    return candidate_solutions, step_size, path_norm, step_norm, cos_thetas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def gradient_descent(gradient, x0, lr=0.1, iterations=100):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to minimize a function.\n",
    "\n",
    "    :param gradient: Function that computes the gradient ∇f(x).\n",
    "    :param x0: Initial guess (NumPy array).\n",
    "    :param lr: Learning rate (step size).\n",
    "    :param iterations: Number of iterations.\n",
    "    :return: Final optimized value of x.\n",
    "    \"\"\"\n",
    "    x = x0  # Initialize x\n",
    "    dim = len(x0)  # Get dimension of search space\n",
    "\n",
    "    # Initialize path variable, decay and damping factors for Cumulative LeArning Rate Adaptation (CLARA)\n",
    "    path = np.zeros(dim)\n",
    "    cos_theta = 0\n",
    "    c = 0.2\n",
    "    d = 1e-3\n",
    "\n",
    "    # Initialize return variables\n",
    "    candidate_solutions = []\n",
    "    learning_rate = []\n",
    "    path_norm = []\n",
    "    gradient_norm = []\n",
    "    cos_thetas = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        grad = gradient(x)  # Compute gradient\n",
    "        x = x - lr * grad  # Update step\n",
    "\n",
    "        # Update mean estimate of cosine of the angle change between current and mean gradient\n",
    "        cos_theta = (1 - c) * cos_theta + np.sqrt(c * (2 - c)) * np.dot(grad, path) / (np.linalg.norm(path) * np.linalg.norm(grad) + 1e-8)\n",
    "\n",
    "        # Update path\n",
    "        path = (1 - c) * path + np.sqrt(c * (2 - c)) * grad\n",
    "\n",
    "        # Update learning rate\n",
    "        lr = lr * np.exp(d * (np.linalg.norm(path)**2 / dim - 1))\n",
    "\n",
    "        candidate_solutions.append(x)\n",
    "        learning_rate.append(lr)\n",
    "        path_norm.append(np.linalg.norm(path)**2)\n",
    "        gradient_norm.append(np.linalg.norm(grad)**2)\n",
    "        cos_thetas.append(cos_theta)\n",
    "\n",
    "    print('Optimized x: ', x)\n",
    "\n",
    "    return candidate_solutions, learning_rate, path_norm, gradient_norm, cos_thetas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def estimate_adam_step_norm(n_steps=1, dim=2, beta1=0.9, beta2=0.999, num_samples=1000):\n",
    "    \"\"\"\n",
    "    Estimates the expectation of the norm of the Adam step using Monte Carlo simulation.\n",
    "\n",
    "    Parameters:\n",
    "    n_steps (int): Number of steps to simulate for\n",
    "    dim (int): Dimensionality of the vectors.\n",
    "    beta1 (float): Adam optimizer's first moment decay rate.\n",
    "    beta2 (float): Adam optimizer's second moment decay rate.\n",
    "    num_samples (int): Number of Monte Carlo samples.\n",
    "\n",
    "    Returns:\n",
    "    float: Estimated expectation of the norm of step_adam.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate samples and compute step_adam norm\n",
    "    step_adam_norm = np.zeros(n_steps)\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        m = np.random.randn(dim)\n",
    "        v = np.random.randn(dim)**2\n",
    "        for i in range(n_steps):\n",
    "            grad = np.random.randn(dim)  # i.i.d. standard normal vector\n",
    "            m = beta1 * m + (1 - beta1) * grad\n",
    "            m_hat = m / (1 - beta1**(i + 1))\n",
    "            v = beta2 * v + (1 - beta2) * grad**2\n",
    "            v_hat = v / (1 - beta2**(i + 1))\n",
    "            step_adam = m_hat / (np.sqrt(v_hat) + 1e-8)\n",
    "            step_adam_norm[i] += np.linalg.norm(step_adam)\n",
    "\n",
    "    # Compute expectation\n",
    "    return step_adam_norm / num_samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'Monte Carlo estimate of $\\\\|\\\\mathbb{E}(\\\\hat{p}_t)\\\\|$')"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_step_monte_carlo = estimate_adam_step_norm(n_steps=10000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# Create vertical subplots (2 rows, 1 column)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "# First subplot: Line plot of the values\n",
    "axes[0].semilogy(norm_step_monte_carlo, color='blue', alpha=0.7)\n",
    "axes[0].set_title(r'Monte Carlo estimate of $\\|\\mathbb{E}(\\hat{p}_t)\\|$')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel(r'$\\|\\mathbb{E}(\\hat{p}_t)\\|$')\n",
    "axes[0].grid()\n",
    "\n",
    "# Second subplot: KDE plot\n",
    "sns.kdeplot(norm_step_monte_carlo, bw_adjust=0.5, ax=axes[1], color=\"red\")\n",
    "axes[1].set_title('KDE Estimate')\n",
    "axes[1].set_xlabel(r'$\\|\\mathbb{E}(\\hat{p}_t)\\|$')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].grid()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "def adam(gradient, x0, lr=0.1, iterations=100, adapt_lr=True):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to minimize a function.\n",
    "\n",
    "    :param gradient: Function that computes the gradient ∇f(x).\n",
    "    :param x0: Initial guess (NumPy array).\n",
    "    :param lr: Learning rate (step size).\n",
    "    :param iterations: Number of iterations.\n",
    "    :return: Final optimized value of x.\n",
    "    \"\"\"\n",
    "    x = x0  # Initialize x\n",
    "    dim = len(x0)  # Get dimension of search space\n",
    "\n",
    "    # Initialize path variable, decay and damping factors for Cumulative LeArning Rate Adaptation (CLARA)\n",
    "    m = np.zeros(dim)\n",
    "    v = np.zeros(dim)\n",
    "    cos_theta = 0\n",
    "    c = 0.2\n",
    "    d = 1e-3\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "\n",
    "    # Estimate of average of cos(angle) between 2 Gaussian vectors\n",
    "    # est_angle = average_angle_gaussian_vectors(dim)\n",
    "\n",
    "    # Initialize return variables\n",
    "    candidate_solutions = []\n",
    "    learning_rate = []\n",
    "    step_norm = []\n",
    "    gradient_norm = []\n",
    "    cos_thetas = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        grad = gradient(x)  # Compute gradient\n",
    "\n",
    "        # Update step\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        m_hat = m / (1 - beta1**(i + 1))\n",
    "        v = beta2 * v + (1 - beta2) * grad**2\n",
    "        v_hat = v / (1 - beta2**(i + 1))\n",
    "        step_adam = m_hat / (np.sqrt(v_hat) + 1e-8)\n",
    "\n",
    "        # Update solution\n",
    "        x = x - lr * step_adam\n",
    "\n",
    "        # Update mean estimate of cosine of the angle change between current and mean gradient\n",
    "        cos_theta = (1 - c) * cos_theta + np.sqrt(c * (2 - c)) * np.dot(grad, step_adam) / (np.linalg.norm(step_adam) * np.linalg.norm(grad) + 1e-8)\n",
    "\n",
    "        # Update learning rate\n",
    "        if adapt_lr:\n",
    "            lr = lr * np.exp(d * (np.linalg.norm(step_adam) / norm_step_monte_carlo[i] - 1))\n",
    "\n",
    "        candidate_solutions.append(x)\n",
    "        learning_rate.append(lr)\n",
    "        step_norm.append(np.linalg.norm(step_adam))\n",
    "        gradient_norm.append(np.linalg.norm(grad))\n",
    "        cos_thetas.append(cos_theta)\n",
    "\n",
    "    print('Optimized x: ', x)\n",
    "\n",
    "    return candidate_solutions, learning_rate, step_norm, gradient_norm, cos_thetas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x:  [ 1.66521074e-101 -2.53335197e-099]\n"
     ]
    }
   ],
   "source": [
    "dim = 2\n",
    "x0 = np.ones(dim)\n",
    "lr0 = 1e-3\n",
    "budget = 10000\n",
    "adapt_lr = False\n",
    "\n",
    "choice = 1\n",
    "\n",
    "if choice == 0:  # Vanilla gradient descent\n",
    "    optimizer = gradient_descent\n",
    "    f = sphere_gradient\n",
    "    fig_title = 'Vanilla gradient descent'  # '(mu, lambda)-ES'\n",
    "elif choice == 1:  # Adam\n",
    "    optimizer = adam\n",
    "    f = sphere_gradient\n",
    "    fig_title = 'Adam'\n",
    "else:  # Evolution strategy\n",
    "    optimizer = evolution_strategy\n",
    "    f = sphere\n",
    "    fig_title = '(mu, lambda)-ES'\n",
    "\n",
    "candidate_sol, learning_rates, path_norm, grad_norm, cos_theta = optimizer(f, x0, lr0, iterations=budget, adapt_lr=adapt_lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "results = [[np.linalg.norm(x) for x in candidate_sol],\n",
    "           learning_rates,\n",
    "           path_norm,\n",
    "           grad_norm,\n",
    "           [path_norm[i] / norm_step_monte_carlo[i] for i in range(len(path_norm))],  # grad_norm[i]\n",
    "           cos_theta\n",
    "           ]\n",
    "fig_titles = ['Distance to optimum',\n",
    "              'Learning rate',\n",
    "              'Path norm',\n",
    "              'Gradient/step norm',\n",
    "              'Path-Monte Carlo estimate ratio',\n",
    "              'Angle between path and current grad./step'\n",
    "              ]\n",
    "y_labels = [r'$\\|x_t\\|$',\n",
    "            r'$\\alpha_t$',\n",
    "            r'$\\|p_t\\|$',\n",
    "            r'$\\|g_t\\|$',\n",
    "            r'$\\|p_t\\| / \\|\\hat{p}_t\\|^2$',\n",
    "            r'$\\cos(\\theta)$'\n",
    "            ]\n",
    "colors = ['r',\n",
    "          'g',\n",
    "          'b',\n",
    "          'c',\n",
    "          'm',\n",
    "          'y'\n",
    "          ]\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 12))  # 2 rows, 2 columns\n",
    "fig.suptitle(f'{fig_title} on the noisy sphere, D = {str(len(x0))}, lr0 = {lr0:.1e}')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):  # Iterate over subplots\n",
    "    if i == len(results) - 1:\n",
    "        ax.plot(results[i], color=colors[i])\n",
    "    else:\n",
    "        ax.semilogy(results[i], color=colors[i])\n",
    "    ax.set_title(fig_titles[i])\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel(y_labels[i])\n",
    "    ax.grid(True)\n",
    "\n",
    "# Adjust layout and show\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
