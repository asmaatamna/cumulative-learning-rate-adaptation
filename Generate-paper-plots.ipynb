{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib tk\n",
    "import ast\n",
    "import re\n",
    "import json\n",
    "from tabulate import tabulate\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "ppo_experiments_dir = os.path.join(current_dir, \"ppo_experiments\")\n",
    "sys.path.append(ppo_experiments_dir)\n",
    "\n",
    "from ppo_experiments.utilities import plot_multiple_results, read_monitor_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "gymnasium_envs = ['Acrobot-v1',\n",
    "                  'Pendulum-v1',\n",
    "                  'LunarLander-v3',\n",
    "                  'BipedalWalker-v3',\n",
    "                  # 'Pong-v5',\n",
    "                  # 'Ant-v5',\n",
    "                  # 'Humanoid-v5'\n",
    "                  ]\n",
    "n_models = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_best_and_median_policy_results(experiment_dir):\n",
    "    \"\"\"\n",
    "    Retrieve cumul. reward and episode length statistics on test environments for the best and median policies from log files\n",
    "    generated during the experiment.\n",
    "    :param experiment_dir: Path to experiment directory\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse experiment path to extract env. config.\n",
    "    match = re.search(r'(\\d+)containers_(\\d+)presses', experiment_dir)\n",
    "    env_name = None\n",
    "\n",
    "    if match:\n",
    "        num_containers = int(match.group(1))\n",
    "        num_presses = int(match.group(2))\n",
    "        env_name = 'CG_n' + str(num_containers) + '_m' + str(num_presses)\n",
    "        # print(f'Containers: {num_containers}, Presses: {num_presses}')\n",
    "    else:\n",
    "        for env in gymnasium_envs:\n",
    "            if env in experiment_dir:\n",
    "                match = True\n",
    "                env_name = env\n",
    "                break\n",
    "\n",
    "    if not match:\n",
    "        print('Invalid experiment directory.')\n",
    "        return False\n",
    "\n",
    "    with open(experiment_dir + 'test_results.txt', 'r') as file:  # TODO: Include '/' in experiment_dir or '/test_results.txt'?\n",
    "        data = file.read()\n",
    "\n",
    "    # Replace numpy float64 with native python floats in data string\n",
    "    data = re.sub(r'np\\.float64\\(([-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?)\\)', r'\\1', data)\n",
    "\n",
    "    # Extracting the list of models from the data\n",
    "    models_str = data.split('Mean and std dev of all ' + str(n_models) + ' models:\\n')[1].split('Best model')[0].strip()\n",
    "    models = ast.literal_eval(models_str)\n",
    "\n",
    "    # Extract best and median seeds from the file\n",
    "    best_seed_match = re.search(r'Best model \\(seed=(\\d+)\\)', data)\n",
    "    median_seed_match = re.search(r'Median model \\(seed=(\\d+)\\)', data)\n",
    "\n",
    "    best_seed = int(best_seed_match.group(1))\n",
    "    median_seed = int(median_seed_match.group(1))\n",
    "\n",
    "    best_model_res = [x[1] for x in models if x[0] == best_seed][0]\n",
    "    median_model_res = [x[1] for x in models if x[0] == median_seed][0]\n",
    "\n",
    "    return env_name, best_seed, best_model_res, median_seed, median_model_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def plot_best_and_median_lr(best_policy_dir, median_policy_dir, title=None, fig_name=None):\n",
    "    \"\"\"\n",
    "    Plot best and median policy training learning rates. Relevant in particular when an adaptive learning rate strategy is used by the optimizer.\n",
    "    :param best_policy_dir: Best policy directory\n",
    "    :param median_policy_dir: Median policy directory\n",
    "    :param title: Desired plot title\n",
    "    :param fig_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df1 = pd.read_csv(best_policy_dir + 'learning_rates.csv')\n",
    "    df2 = pd.read_csv(median_policy_dir + 'learning_rates.csv')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.semilogy(df1.iloc[:, 0], df1.iloc[:, 1], linestyle='-', color='b', label='Best')\n",
    "    ax.semilogy(df2.iloc[:, 0], df2.iloc[:, 1], linestyle='--', color='r', label='Median')\n",
    "\n",
    "    ax.set_xlabel('Timesteps')\n",
    "    ax.set_ylabel('Learning rate')\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    else:\n",
    "        ax.set_title(\"Training learning rate\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    if fig_name:\n",
    "        fig.savefig(f'{fig_name}.pdf')\n",
    "\n",
    "    return fig"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exponential_smoothing(arrays, alpha=0.2):\n",
    "    \"\"\"Applies exponential smoothing to a list of NumPy arrays.\"\"\"\n",
    "    smoothed = [arrays[0]]  # Initialize with first value\n",
    "\n",
    "    for i in range(1, len(arrays)):\n",
    "        smoothed_value = (1 - alpha) * smoothed[-1] + np.sqrt(alpha * (2 - alpha)) * arrays[i]\n",
    "        smoothed.append(smoothed_value)\n",
    "\n",
    "    return smoothed  # np.array(smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_cumul_path_distrib_mean_vs_dim(l_dim, l_peaks, title=None):\n",
    "    \"\"\"\n",
    "    :param l_dim: list of #params to plot against (dimensionality of the problem)\n",
    "    :param l_peaks: list of peaks of \\|p_t\\| values for median seeds/models\n",
    "    :param title: plot title\n",
    "    :return: figure\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    # for dim, peaks in zip(l_dim, l_peaks):  # If l_peaks is a list of lists\n",
    "    #     ax.scatter([dim] * len(peaks), peaks)\n",
    "\n",
    "    ax.plot(l_dim, l_peaks, marker='o',  markeredgewidth=2, linewidth=3)\n",
    "    ax.set_xticks(l_dim)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(r'$D$')\n",
    "    ax.set_ylabel(r'Mean of $\\|p_t\\|$ distribution (median seed)')\n",
    "    ax.grid(True)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_adam_steps(experiment_dir, title=None, c=0.1, fig_name=None):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 9))\n",
    "    adam_step_label = r'$\\frac{\\|\\hat{m}_t / (\\sqrt{\\hat{v_t}} + \\epsilon)\\|^2}{\\nu_t D}$'\n",
    "    if title:\n",
    "        axes[0].set_title(title)\n",
    "    axes[0].set_xlabel('Timesteps')\n",
    "    axes[0].set_ylabel(adam_step_label)\n",
    "\n",
    "    axes[1].set_title(r'Distribution of ' + adam_step_label)\n",
    "    axes[1].set_xlabel(adam_step_label)\n",
    "\n",
    "    axes[2].set_xlabel('Timesteps')\n",
    "    axes[2].set_ylabel('Learning rate')\n",
    "\n",
    "    _, best_seed, _, median_seed, _ = get_best_and_median_policy_results(experiment_dir)\n",
    "\n",
    "    density_plot = None\n",
    "\n",
    "    for seed in range(n_models):\n",
    "        with open(f\"{experiment_dir}/{seed:02d}/adam_updates.json\", 'r') as f:\n",
    "             data = json.load(f)  # Load JSON content\n",
    "        df = pd.DataFrame(data)\n",
    "        column = 'adam_update'\n",
    "        df[column] = df[column].apply(np.array)\n",
    "        D = len(df[column].iloc[-1])\n",
    "\n",
    "        df['path'] = exponential_smoothing(df[column].to_list(), alpha=c)\n",
    "        df['path_norm'] = df['path'].apply(lambda x: np.linalg.norm(x)**2)\n",
    "        df['step_norm'] = df[column].apply(lambda x: np.linalg.norm(x)**2)\n",
    "        df['normalization'] = df['path_norm'] / D  # (df['path_norm'] - df['step_norm']) / D\n",
    "\n",
    "        df_lr = pd.read_csv(f\"{experiment_dir}/{seed:02d}/\" + 'learning_rates.csv')\n",
    "\n",
    "        if seed == best_seed:\n",
    "            axes[0].semilogy(df['timestep'], df['normalization'], label='Best')\n",
    "            density_plot = df['normalization'].plot.kde(ax=axes[1], label='Best')\n",
    "            axes[2].semilogy(df_lr['timestep'], df_lr['learning_rate'], label='Best')\n",
    "\n",
    "        elif seed == median_seed:\n",
    "            axes[0].semilogy(df['timestep'], df['normalization'], label='Median')\n",
    "            density_plot = df['normalization'].plot.kde(ax=axes[1], label='Median')\n",
    "            axes[2].semilogy(df_lr['timestep'], df_lr['learning_rate'], label='Median')\n",
    "        else:\n",
    "            axes[0].semilogy(df['timestep'], df['normalization'])\n",
    "            density_plot = df['normalization'].plot.kde(ax=axes[1], label='_')\n",
    "            axes[2].semilogy(df_lr['timestep'], df_lr['learning_rate'])\n",
    "\n",
    "    # Extract peaks of path norm density plots\n",
    "    line = density_plot.get_lines()[median_seed]\n",
    "    x, y = line.get_xdata(), line.get_ydata()\n",
    "\n",
    "    # Find the x value where the density is highest\n",
    "    density_peak = x[np.argmax(y)]\n",
    "\n",
    "    axes[0].grid(True), axes[1].grid(True), axes[2].grid(True)\n",
    "    axes[0].legend(), axes[1].legend(), axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    if fig_name:\n",
    "        fig.savefig(f'{fig_name}.pdf')\n",
    "\n",
    "    return fig, D, density_peak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_and_save_linear_model(x, y, model_path='linear_model.pkl', x_label=r'$D$', y_label='Peak of ' + r'$\\frac{\\|\\hat{m}_t / (\\sqrt{\\hat{v_t}} + \\epsilon)\\|^2}{\\sigma^2 D}$' + ' distribution (median seed)'):\n",
    "    \"\"\"\n",
    "    Fits a linear regression model to the given data, saves it to a file, and returns the trained model.\n",
    "    \"\"\"\n",
    "    x = np.array(x).reshape(-1, 1)  # Ensure x is a column vector\n",
    "    y = np.array(y)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(x, y)\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "    print(f'Model saved to {model_path}')\n",
    "\n",
    "     # Predict y-values for the given x\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    # Plot the results\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Linear approximation')\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.plot(x, y, color='blue', label='Data', marker='o', linestyle='--')\n",
    "    ax.plot(x, y_pred, color='red', linewidth=2, label='Fitted line')\n",
    "    ax.set_xticks(x.ravel())\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "# x = [1, 2, 3, 4, 5]\n",
    "# y = [2.2, 2.8, 3.6, 4.5, 5.1]\n",
    "\n",
    "# model = fit_and_save_linear_model(x, y)\n",
    "\n",
    "# Load the model later for use\n",
    "# loaded_model = joblib.load('linear_model.pkl')\n",
    "# print(f\"Loaded Model Coefficients: {loaded_model.coef_[0]}, Intercept: {loaded_model.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_results(experiments_dir, plot_figs=True):\n",
    "    # TODO: Adjust y-label on best & median learning rate plots\n",
    "    env_configs = []  # Labels for env. configurations\n",
    "    configs_dir = []\n",
    "    if 'containergym' in experiments_dir:\n",
    "        n_container_values = [5, 5, 11, 11]\n",
    "        n_pu_values = [2, 5, 2, 11]\n",
    "        for n, m in zip(n_container_values, n_pu_values):\n",
    "             env_configs.append('n' + str(n) + '_m' + str(m))\n",
    "             configs_dir.append(str(n) + 'containers_' + str(m) + 'presses_timestep_2min/')\n",
    "    else:\n",
    "        env_configs = gymnasium_envs\n",
    "\n",
    "    # Best model stats\n",
    "    best_rewards = []\n",
    "    best_std_rewards = []\n",
    "    best_lengths = []\n",
    "    best_std_lengths = []\n",
    "\n",
    "    # Median model stats\n",
    "    median_rewards = []\n",
    "    median_std_rewards = []\n",
    "    median_lengths = []\n",
    "    median_std_lengths = []\n",
    "\n",
    "    l_dim = []\n",
    "    l_peaks = []\n",
    "\n",
    "    for i in range(len(env_configs)):\n",
    "        if 'containergym' in experiments_dir:\n",
    "            path = experiments_dir + configs_dir[i]\n",
    "        else:\n",
    "            path = experiments_dir + env_configs[i] + '/'\n",
    "        _, best_seed, best, median_seed, median = get_best_and_median_policy_results(path)\n",
    "        best_rewards.append(best['avg_reward']), best_std_rewards.append(best['std_reward'])\n",
    "        best_lengths.append(best['avg_length']), best_std_lengths.append(best['std_length'])\n",
    "\n",
    "        median_rewards.append(median['avg_reward']), median_std_rewards.append(median['std_reward'])\n",
    "        median_lengths.append(median['avg_length']), median_std_lengths.append(median['std_length'])\n",
    "\n",
    "        # Plot training learning rates\n",
    "        if 'dadaptation' in experiments_dir or 'prodigy' in experiments_dir:\n",
    "            plot_best_and_median_lr(path + '{:02}'.format(best_seed) + '/', path + '{:02}'.format(median_seed) + '/', env_configs[i],\n",
    "                                    fig_name=experiments_dir + 'best_and_median_lr_' + env_configs[i])\n",
    "\n",
    "        # Plot learning curves (training return) per env. config.\n",
    "        log_dirs = []\n",
    "        for seed in range(n_models):\n",
    "            log_dir = f'{path}/{seed:02d}/'\n",
    "            log_dirs.append(log_dir)\n",
    "\n",
    "        plot_multiple_results(log_dirs, save_location=experiments_dir, title='Smoothed training rewards on ' + env_configs[i], window_size=50)\n",
    "\n",
    "        # Plot gradient norms\n",
    "        if 'dadaptation' not in experiments_dir and 'prodigy' not in experiments_dir:\n",
    "            _, D, density_peaks = plot_adam_steps(path, title='Norm of Adam steps on ' + env_configs[i], fig_name=experiments_dir + 'norm_adam_steps_' + env_configs[i])\n",
    "\n",
    "            # Prepare lists for path's highest density plots\n",
    "            l_dim.append(D)\n",
    "            l_peaks.append(density_peaks)\n",
    "\n",
    "    # Train linear model for path mean value as a function of a model's #params\n",
    "    if 'dadaptation' not in experiments_dir and 'prodigy' not in experiments_dir:\n",
    "        _ = fit_and_save_linear_model(l_dim, l_peaks)\n",
    "\n",
    "    # Print test results for best and median policies\n",
    "    print('Best policy statistics:')\n",
    "    headers = ['Config.', 'Cumul. r', 'Std. cumul. r', 'Episode length', 'Std. episode length']\n",
    "    table = zip(env_configs, best_rewards, best_std_rewards, best_lengths, best_std_lengths)\n",
    "    print(tabulate(table, headers=headers, floatfmt='.2f'))\n",
    "\n",
    "    print('\\n')\n",
    "    print('Median policy statistics:')\n",
    "    headers = ['Config.', 'Cumul. r', 'Std. cumul. r', 'Episode length', 'Std. episode length']\n",
    "    table = zip(env_configs, median_rewards, median_std_rewards, median_lengths, median_std_lengths)\n",
    "    print(tabulate(table, headers=headers, floatfmt='.2f'))\n",
    "\n",
    "    x = np.arange(len(env_configs))  # Position for bars\n",
    "    width = 0.35  # Width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # ----- BAR PLOT: Reward Comparison -----\n",
    "    ax[0].bar(x - width/2, best_rewards, width, yerr=best_std_rewards, label='Best', capsize=5, color='b', alpha=0.7)\n",
    "    ax[0].bar(x + width/2, median_rewards, width, yerr=median_std_rewards, label='Median', capsize=5, color='r', alpha=0.7)\n",
    "    ax[0].set_ylabel('Cumulative reward')\n",
    "    ax[0].set_title('Best vs. median policy - Cumulative reward')\n",
    "    ax[0].grid(True)\n",
    "    ax[0].set_xticks(x)\n",
    "    ax[0].set_xticklabels(env_configs)\n",
    "    ax[0].legend()\n",
    "\n",
    "    # ----- LINE PLOT: Episode Length Comparison -----\n",
    "    ax[1].errorbar(env_configs, best_lengths, yerr=best_std_lengths, label='Best', marker='o', linestyle='-', color='b', capsize=5)\n",
    "    ax[1].errorbar(env_configs, median_lengths, yerr=median_std_lengths, label='Median', marker='D', linestyle='-', color='r', capsize=5)\n",
    "    ax[1].set_ylabel('Episode length')\n",
    "    ax[1].set_ylim(0)\n",
    "    ax[1].grid(True)\n",
    "    ax[1].set_title('Best vs. median policy - Episode length')\n",
    "    ax[1].legend()\n",
    "\n",
    "    # plot_cumul_path_distrib_mean_vs_dim(l_dim, l_peaks)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(experiments_dir + 'reward_plots.pdf')\n",
    "\n",
    "    if plot_figs:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def compute_mean_std_array(x_arrays, y_arrays, common_x=None, label='Mean', color='blue', alpha=0.3):\n",
    "    \"\"\"\n",
    "    Interpolates multiple (x, y) arrays to a common x-axis, computes the mean and standard deviation,\n",
    "    and plots the result with a shaded area for variability.\n",
    "\n",
    "    Parameters:\n",
    "    - x_arrays: List of x-axis arrays.\n",
    "    - y_arrays: List of corresponding y-axis arrays.\n",
    "    - common_x: Optional array defining the common x-axis. If None, it will be auto-generated.\n",
    "    - label: Label for the mean line.\n",
    "    - color: Color for the plot line and shaded area.\n",
    "    - alpha: Transparency level for the shaded area.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate input\n",
    "    if len(x_arrays) != len(y_arrays):\n",
    "        raise ValueError(\"x_arrays and y_arrays must have the same length.\")\n",
    "\n",
    "    # Automatically define a common x-axis if not provided\n",
    "    if common_x is None:\n",
    "        min_x = min([x.min() for x in x_arrays])\n",
    "        max_x = max([x.max() for x in x_arrays])\n",
    "        common_x = np.linspace(min_x, max_x, max([len(x) for x in x_arrays]))\n",
    "\n",
    "    # Interpolate each (x, y) pair to the common x-axis\n",
    "    interpolated_ys = [np.interp(common_x, x, y) for x, y in zip(x_arrays, y_arrays)]\n",
    "\n",
    "    # Stack and compute mean and standard deviation\n",
    "    stacked = np.vstack(interpolated_ys)\n",
    "    mean_y = np.mean(stacked, axis=0)\n",
    "    std_y = np.std(stacked, axis=0)\n",
    "\n",
    "    return common_x, mean_y, std_y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def plot_learning_curves_on_env_type(env_type, list_experiment_dir, color_code_optimizers=None, fig_location='./figures/'):\n",
    "    # env_type = 'containergym' or 'gymnasium'\n",
    "\n",
    "    # Extract environment type from dir\n",
    "    if all(env_type in experiment_dir for experiment_dir in list_experiment_dir):\n",
    "        env_configs = []  # Labels for env. configurations\n",
    "        configs_dir = []\n",
    "        if env_type == 'containergym':\n",
    "            n_container_values = [5, 5, 11, 11]\n",
    "            n_pu_values = [2, 5, 2, 11]\n",
    "            for n, m in zip(n_container_values, n_pu_values):\n",
    "                 env_configs.append('CG_n' + str(n) + '_m' + str(m))\n",
    "                 configs_dir.append(str(n) + 'containers_' + str(m) + 'presses_timestep_2min/')\n",
    "        else:\n",
    "            env_configs = gymnasium_envs\n",
    "            configs_dir = [env + '/' for env in gymnasium_envs]\n",
    "\n",
    "        # Create dictionary of results to plot\n",
    "        res_dict = dict.fromkeys(env_configs, None)\n",
    "\n",
    "        # Create color-to-optimizer mapping\n",
    "        color_dict = dict()\n",
    "        linestyles = ['--', ':', '--', (0, (1, 10))]\n",
    "\n",
    "        for i in range(len(env_configs)):  # Loop over environments\n",
    "            # list_df_lr = dict()\n",
    "            res_alg = dict()\n",
    "            for j, experiment_dir in enumerate(list_experiment_dir):  # Loop over algorithms\n",
    "                # Extract algorithm's name\n",
    "                alg_name = None\n",
    "                if 'adam' in experiment_dir:\n",
    "                    alg_name = 'Adam'\n",
    "                    if 'adaptive_lr' in experiment_dir:\n",
    "                        alg_name += '_CLARA'\n",
    "                    if 'linear_schedule' in experiment_dir:\n",
    "                        alg_name += '_LS'\n",
    "                if 'dadaptation' in experiment_dir:\n",
    "                    alg_name = 'Adam_D-Adaptation'\n",
    "\n",
    "                 # Map color to optimizer/algorithm\n",
    "                color_dict[alg_name] = color_code_optimizers[alg_name]\n",
    "\n",
    "                path = experiment_dir + configs_dir[i]\n",
    "                _, best_seed, _, median_seed, _ = get_best_and_median_policy_results(path)\n",
    "\n",
    "                # Collect learning rates for all seeds\n",
    "                dict_lr = dict()\n",
    "                for seed in range(n_models):\n",
    "                    if seed == best_seed:\n",
    "                        dict_lr['best'] = pd.read_csv(f\"{path}/{seed:02d}/\" + 'learning_rates.csv')\n",
    "                    elif seed == median_seed:\n",
    "                        dict_lr['median'] = pd.read_csv(f\"{path}/{seed:02d}/\" + 'learning_rates.csv')\n",
    "                    else:\n",
    "                        dict_lr[seed] = pd.read_csv(f\"{path}/{seed:02d}/\" + 'learning_rates.csv')\n",
    "\n",
    "                x_arrays = []\n",
    "                y_arrays = []\n",
    "                for s in range(n_models):\n",
    "                    x, y = read_monitor_file(f'{path}/{s:02d}/', window_size=25)\n",
    "                    x_arrays.append(x)\n",
    "                    y_arrays.append(y)\n",
    "\n",
    "                res_alg[alg_name] = (compute_mean_std_array(x_arrays, y_arrays), dict_lr)\n",
    "\n",
    "            res_dict[env_configs[i]] = res_alg\n",
    "\n",
    "        # Plot best and median training rewards for each algorithm per environment\n",
    "        for env in res_dict:\n",
    "            # Plot training rewards\n",
    "            fig_reward, ax_reward = plt.subplots(1, 1, sharey=True, figsize=(6, 5))\n",
    "            # Plot learning rates\n",
    "            fig_lr, ax_lr = plt.subplots(1, len(color_dict), figsize=(len(color_dict) * 6, 5))\n",
    "\n",
    "            for j, alg in enumerate(res_dict[env]):\n",
    "                common_x, mean_y, std_y = res_dict[env][alg][0]\n",
    "                ax_reward.plot(common_x, mean_y, color=color_dict[alg], label=alg)\n",
    "                ax_reward.fill_between(common_x, mean_y - std_y, mean_y + std_y, color=color_dict[alg], alpha=0.3)\n",
    "                ls_idx = -1\n",
    "                label = None\n",
    "                for k in res_dict[env][alg][-1]:\n",
    "                    if k == 'best':\n",
    "                        linestyle = '-'\n",
    "                        label = 'Best'\n",
    "                    elif k == 'median':\n",
    "                        linestyle = '-.'\n",
    "                        label = 'Median'\n",
    "                    else:\n",
    "                        ls_idx += 1\n",
    "                        linestyle = linestyles[ls_idx]\n",
    "                        label = None\n",
    "\n",
    "                    alpha = 1 if k == 'best' or k == 'median' else 0.2\n",
    "                    ax_lr[j].semilogy(res_dict[env][alg][-1][k]['timestep'], res_dict[env][alg][-1][k]['learning_rate'], color=color_dict[alg], label=label, linestyle=linestyle, alpha=alpha)\n",
    "\n",
    "                ax_lr[j].set_title(alg)\n",
    "                ax_lr[j].set_xlabel('Timesteps')\n",
    "                ax_lr[j].set_ylabel('Learning rate')\n",
    "                ax_lr[j].legend(loc='lower right')\n",
    "                ax_lr[j].grid(True)\n",
    "\n",
    "            ax_reward.legend(loc='lower right')\n",
    "            ax_reward.grid(True)\n",
    "            ax_reward.set_xlabel('Timesteps')\n",
    "            ax_reward.set_ylabel('Average episodic return')\n",
    "            fig_reward.suptitle(env + ' - Average episodic return', fontsize=14)\n",
    "\n",
    "            fig_lr.suptitle(env + ' - Learning rate', fontsize=14)\n",
    "            figname = env + '_training_reward'\n",
    "\n",
    "            plt.tight_layout()\n",
    "            fig_reward.savefig(fig_location + figname + '.pdf')\n",
    "            figname = env + '_learning_rate'\n",
    "            fig_lr.savefig(fig_location + figname + '.pdf')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def plot_test_reward_on_env_type(env_type, list_experiment_dir, color_code_optimizers=None, fig_location='./figures/'):\n",
    "    # env_type = 'containergym' or 'gymnasium'\n",
    "\n",
    "    # Extract environment type from dir\n",
    "    if all(env_type in experiment_dir for experiment_dir in list_experiment_dir):\n",
    "        env_configs = []  # Labels for env. configurations\n",
    "        configs_dir = []\n",
    "        if env_type == 'containergym':\n",
    "            n_container_values = [5, 5, 11, 11]\n",
    "            n_pu_values = [2, 5, 2, 11]\n",
    "            for n, m in zip(n_container_values, n_pu_values):\n",
    "                 env_configs.append('CG_n' + str(n) + '_m' + str(m))\n",
    "                 configs_dir.append(str(n) + 'containers_' + str(m) + 'presses_timestep_2min/')\n",
    "        else:\n",
    "            env_configs = gymnasium_envs\n",
    "            configs_dir = [env + '/' for env in gymnasium_envs]\n",
    "\n",
    "        # Create dictionary of results to plot\n",
    "        res_dict = dict.fromkeys(env_configs, None)\n",
    "\n",
    "        # Create color-to-optimizer mapping\n",
    "        color_dict = dict()\n",
    "\n",
    "        for i in range(len(env_configs)):  # Loop over environments\n",
    "            res_alg = dict()\n",
    "            for j, experiment_dir in enumerate(list_experiment_dir):  # Loop over algorithms\n",
    "                # Extract algorithm's name\n",
    "                alg_name = None\n",
    "                if 'adam' in experiment_dir:\n",
    "                    alg_name = 'Adam'\n",
    "                    if 'adaptive_lr' in experiment_dir:\n",
    "                        alg_name += '_CLARA'\n",
    "                    if 'linear_schedule' in experiment_dir:\n",
    "                        alg_name += '_LS'\n",
    "                if 'dadaptation' in experiment_dir:\n",
    "                    alg_name = 'Adam_D-Adaptation'\n",
    "\n",
    "                 # Map color to optimizer/algorithm\n",
    "                color_dict[alg_name] = color_code_optimizers[alg_name]\n",
    "\n",
    "                path = experiment_dir + configs_dir[i]\n",
    "                _, best_seed, best, median_seed, median = get_best_and_median_policy_results(path)\n",
    "                res_alg[alg_name] = (best, median)\n",
    "\n",
    "            res_dict[env_configs[i]] = res_alg\n",
    "\n",
    "        # Plot best and median test rewards for each algorithm per environment\n",
    "        for env in res_dict:\n",
    "            best_rewards = []\n",
    "            best_std_rewards = []\n",
    "            median_rewards = []\n",
    "            median_std_rewards = []\n",
    "            x = []  # Position for bars\n",
    "            for alg in res_dict[env]:\n",
    "                x.append(alg)\n",
    "                best_rewards.append(res_dict[env][alg][0]['avg_reward']), best_std_rewards.append(res_dict[env][alg][0]['std_reward'])\n",
    "                median_rewards.append(res_dict[env][alg][-1]['avg_reward']), median_std_rewards.append(res_dict[env][alg][-1]['std_reward'])\n",
    "\n",
    "            width = 0.35  # Width of the bars\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "            figname = env + '_test_reward'\n",
    "\n",
    "            # ----- BAR PLOT: Reward Comparison -----\n",
    "            ax.bar(np.arange(len(x)) - width/2, best_rewards, width, yerr=best_std_rewards, capsize=5, color=[color_dict[alg] for alg in x], alpha=0.7)\n",
    "            ax.bar(np.arange(len(x)) + width/2, median_rewards, width, yerr=median_std_rewards, hatch='//', capsize=5, color=[color_dict[alg] for alg in x], alpha=0.7)\n",
    "            ax.set_ylabel('Test episodic return')\n",
    "            ax.set_title(env + ' - Test episodic return')\n",
    "            ax.grid(True)\n",
    "            ax.set_xticks(np.arange(len(x)))\n",
    "            ax.set_xticklabels(x)\n",
    "\n",
    "            legend_patches = [mpatches.Patch(facecolor='gray', label='Best'),\n",
    "                              mpatches.Patch(facecolor='gray', hatch='//', label='Median')\n",
    "                              ]\n",
    "\n",
    "            ax.legend(handles=legend_patches, loc='lower right')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            fig.savefig(fig_location + figname + '.pdf')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def plot_test_reward_per_damping_value(env_type, lr0, damping, experiments_dir='/local/aatamna/', fig_location='./figures/'):\n",
    "    env_configs = []  # Labels for env. configurations\n",
    "    configs_dir = []\n",
    "    if env_type == 'containergym':\n",
    "        n_container_values = [5, 5, 11, 11]\n",
    "        n_pu_values = [2, 5, 2, 11]\n",
    "        for n, m in zip(n_container_values, n_pu_values):\n",
    "             env_configs.append('CG_n' + str(n) + '_m' + str(m))\n",
    "             configs_dir.append(str(n) + 'containers_' + str(m) + 'presses_timestep_2min/')\n",
    "    else:\n",
    "        env_configs = gymnasium_envs\n",
    "        configs_dir = [env + '/' for env in gymnasium_envs]\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(lr0), sharey=True, figsize=(6 * len(lr0), 5))\n",
    "    figname = env_type + '_damping_experiments'\n",
    "    for j in range(len(lr0)):\n",
    "        res_env = dict()\n",
    "        adam_res_env = dict()\n",
    "        for i in range(len(configs_dir)):\n",
    "            best_seed_rewards = []\n",
    "            best_seed_std = []\n",
    "            for d in damping:\n",
    "                path = experiments_dir + 'ppo_adam_adaptive_lr_' + env_type + '_lr0_' + str(lr0[j]) + '_d_' + str(d) + '/' + configs_dir[i]\n",
    "                _, _, best, _, _ = get_best_and_median_policy_results(path)\n",
    "                best_seed_rewards.append(best['avg_reward'])\n",
    "                best_seed_std.append(best['std_reward'])\n",
    "\n",
    "            res_env[env_configs[i]] = (best_seed_rewards, best_seed_std)\n",
    "\n",
    "            # Get standard Adam performance\n",
    "            path_baseline = experiments_dir + 'ppo_adam_' + env_type + '_lr0_' + str(lr0[j]) + '/' + configs_dir[i]\n",
    "            _, _, best_adam, _, _ = get_best_and_median_policy_results(path_baseline)\n",
    "            adam_res_env[env_configs[i]] = best_adam['avg_reward']\n",
    "\n",
    "        # Plot best training average cumul. reward as a function of the damping for each env. config.\n",
    "        for env in res_env:\n",
    "            axes[j].errorbar(damping, res_env[env][0], yerr=res_env[env][1], label=env, linestyle='-', capsize=3)\n",
    "            current_color = axes[j].get_lines()[-1].get_c()\n",
    "            axes[j].hlines(y=adam_res_env[env], xmin=damping[0], xmax=damping[-1], colors=current_color, linestyles='-.', linewidth=3)\n",
    "\n",
    "        # Adding labels and title\n",
    "        fig.suptitle('Adam with CLARA - Best policy - Test episodic return vs. damping')\n",
    "        axes[j].set_xticks(damping)\n",
    "        axes[j].set_xlabel('Damping')\n",
    "        axes[j].set_ylabel('Test episodic return')\n",
    "        axes[j].set_title('lr0 = ' + str(lr0[j]))  # TODO: Update lr0 according to lr notation in paper\n",
    "        # axes[j].set_xlim(damping[0], damping[-1])\n",
    "        handles, labels = axes[j].get_legend_handles_labels()\n",
    "        dotted_line = Line2D([0], [0], color='black', linestyle='-.', label='Adam')\n",
    "        handles.append(dotted_line)\n",
    "        labels.append('Adam')\n",
    "        axes[j].grid(True)\n",
    "\n",
    "    axes[j].legend(handles=handles, labels=labels)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(fig_location + figname + '.pdf')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiments_dir = '/local/aatamna/'  # '/local/aatamna/rl_lr_experiments_5seeds/'\n",
    "env_type = 'containergym'\n",
    "# list_experiments = [experiments_dir + 'ppo_adam_' + env_type + '_lr0_0.0003/',\n",
    "#                     experiments_dir + 'ppo_adam_linear_schedule_' + env_type + '_lr0_0.0003/',\n",
    "#                     experiments_dir + 'ppo_dadaptation_' + env_type + '_lr_coef_0.01/',\n",
    "#                     experiments_dir + 'ppo_adam_adaptive_lr_' + env_type + '_lr0_0.0003/'\n",
    "#                     ]\n",
    "\n",
    "list_experiments = [experiments_dir + 'ppo_adam_' + env_type + '_lr0_0.0003/',\n",
    "                    # experiments_dir + 'ppo_adam_linear_schedule_' + env_type + '_lr0_0.0003/',\n",
    "                    # experiments_dir + 'ppo_dadaptation_' + env_type + '_lr_coef_0.01/',\n",
    "                    experiments_dir + 'ppo_adam_adaptive_lr_' + env_type + '_lr0_0.0003_d_0.05/'\n",
    "                    ]\n",
    "\n",
    "# Number of colors needed for plotting rewards per algorithm\n",
    "num_colors = 4\n",
    "alg_names = ['Adam', 'Adam_LS', 'Adam_D-Adaptation', 'Adam_CLARA']\n",
    "\n",
    "# Use a colormap (e.g., 'tab10', 'viridis', 'plasma')\n",
    "colormap = plt.get_cmap('tab10', num_colors)\n",
    "\n",
    "# Generate a list of colors and map each one to an algorithm name\n",
    "colors = dict()\n",
    "for i in range(len(alg_names)):\n",
    "    colors[alg_names[i]] = colormap(i)\n",
    "\n",
    "plot_test_reward_on_env_type(env_type, list_experiments, colors)\n",
    "plot_learning_curves_on_env_type(env_type, list_experiments, colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/local/aatamna/ppo_adam_linear_schedule_gymnasium/LunarLander-v3/test_results.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdisplay_results\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexperiments_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplot_figs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[11], line 36\u001B[0m, in \u001B[0;36mdisplay_results\u001B[0;34m(experiments_dir, plot_figs)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     35\u001B[0m     path \u001B[38;5;241m=\u001B[39m experiments_dir \u001B[38;5;241m+\u001B[39m env_configs[i] \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m---> 36\u001B[0m _, best_seed, best, median_seed, median \u001B[38;5;241m=\u001B[39m \u001B[43mget_best_and_median_policy_results\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m best_rewards\u001B[38;5;241m.\u001B[39mappend(best[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mavg_reward\u001B[39m\u001B[38;5;124m'\u001B[39m]), best_std_rewards\u001B[38;5;241m.\u001B[39mappend(best[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstd_reward\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     38\u001B[0m best_lengths\u001B[38;5;241m.\u001B[39mappend(best[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mavg_length\u001B[39m\u001B[38;5;124m'\u001B[39m]), best_std_lengths\u001B[38;5;241m.\u001B[39mappend(best[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstd_length\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "Cell \u001B[0;32mIn[4], line 29\u001B[0m, in \u001B[0;36mget_best_and_median_policy_results\u001B[0;34m(experiment_dir)\u001B[0m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInvalid experiment directory.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mexperiment_dir\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtest_results.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m file:  \u001B[38;5;66;03m# TODO: Include '/' in experiment_dir or '/test_results.txt'?\u001B[39;00m\n\u001B[1;32m     30\u001B[0m     data \u001B[38;5;241m=\u001B[39m file\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# Extracting the list of models from the data\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:286\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    279\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    282\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    283\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    284\u001B[0m     )\n\u001B[0;32m--> 286\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/local/aatamna/ppo_adam_linear_schedule_gymnasium/LunarLander-v3/test_results.txt'"
     ]
    }
   ],
   "source": [
    "damping_values = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "plot_test_reward_per_damping_value('containergym', lr0=[1e-6, 3e-4, 1.0], damping=damping_values, experiments_dir='/local/aatamna/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to linear_model.pkl\n",
      "Best policy statistics:\n",
      "Config.      Cumul. r    Std. cumul. r    Episode length    Std. episode length\n",
      "---------  ----------  ---------------  ----------------  ---------------------\n",
      "n5_m2           58.62             1.61            600.00                   0.00\n",
      "n5_m5           55.84             1.79            600.00                   0.00\n",
      "n11_m2          86.68             2.48            600.00                   0.00\n",
      "n11_m11         87.57             1.52            600.00                   0.00\n",
      "\n",
      "\n",
      "Median policy statistics:\n",
      "Config.      Cumul. r    Std. cumul. r    Episode length    Std. episode length\n",
      "---------  ----------  ---------------  ----------------  ---------------------\n",
      "n5_m2           54.38             1.35            600.00                   0.00\n",
      "n5_m5           53.57             1.72            600.00                   0.00\n",
      "n11_m2          85.98             2.89            600.00                   0.00\n",
      "n11_m11         85.62             3.77            600.00                   0.00\n"
     ]
    }
   ],
   "source": [
    "experiments_dir = '/local/aatamna/rl_lr_experiments_5seeds/ppo_adam_containergym_lr0_0.0003/'  # '/local/aatamna/ppo_adam_containergym_lr0_1e-06/'\n",
    "display_results(experiments_dir, plot_figs=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lr_adaptation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
